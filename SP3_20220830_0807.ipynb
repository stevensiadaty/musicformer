{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9opKSK2RSDRg"
   },
   "source": [
    "# Generating music via Neural Net\n",
    "## Piano, single track\n",
    "### Based on Google 2018 Music Transformer NN\n",
    "\n",
    "Codes recycled from:\n",
    "\n",
    "1) Alex https://github.com/asigalov61/SuperPiano/blob/master/Super_Piano_3.ipynb\n",
    "\n",
    "2) Damon https://github.com/gwinndr/MusicTransformer-Pytorch\n",
    "\n",
    "3) Jason https://github.com/jason9693/midi-neural-processor\n",
    "\n",
    "4) Mir https://github.com/mirsiadaty\n",
    "\n",
    "Thank you :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 1.1: initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 29 16:13:51 2022\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 1.2: create dir for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/mnt3p22//sp3_1’: File exists\n",
      "/home/mnt3p22/sp3_1\n",
      "total 8\n",
      "drwxrwxr-x 7 mnt3p22 mnt3p22 4096 Aug 29 16:32 MusicTransformer-Pytorch\n",
      "drwxrwxr-x 4 mnt3p22 mnt3p22 4096 Aug 29 16:29 midi-neural-processor\n",
      "Mon Aug 29 17:46:28 2022\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "YourHomeDir = '/home/mnt3p22/'\n",
    "YourProjectSubDir = 'sp3_1'\n",
    "\n",
    "#\n",
    "!mkdir $YourHomeDir/$YourDir\n",
    "%cd $YourHomeDir/$YourDir\n",
    "!ls -ltA \n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 1.3: Check GPU and driver available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "Ror_UJUp7wlO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "Mon Aug 29 17:47:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:3B:00.0  On |                  Off |\n",
      "| 33%   47C    P0    70W / 260W |   1446MiB / 48600MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1224      G   /usr/lib/xorg/Xorg                214MiB |\n",
      "|    0   N/A  N/A      2010      G   /usr/lib/xorg/Xorg                985MiB |\n",
      "|    0   N/A  N/A      2170      G   /usr/bin/gnome-shell               73MiB |\n",
      "|    0   N/A  N/A      3858      G   /usr/lib/firefox/firefox          161MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Mon Aug 29 17:47:36 2022\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 2.1: clone github lib: Jason's MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'midi-neural-processor'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 27 (delta 0), reused 0 (delta 0), pack-reused 26\u001b[K\n",
      "Unpacking objects: 100% (27/27), 8.56 KiB | 486.00 KiB/s, done.\n",
      "Mon Aug 29 16:29:13 2022\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/asigalov61/midi-neural-processor\n",
    "!git clone https://github.com/jason9693/midi-neural-processor\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 2.2: clone github lib: Gwinn's NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MusicTransformer-Pytorch'...\n",
      "remote: Enumerating objects: 346, done.\u001b[K\n",
      "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 346 (delta 5), reused 13 (delta 5), pack-reused 332\u001b[K\n",
      "Receiving objects: 100% (346/346), 109.42 KiB | 3.65 MiB/s, done.\n",
      "Resolving deltas: 100% (190/190), done.\n",
      "Mon Aug 29 16:32:01 2022\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/asigalov61/MusicTransformer-Pytorch\n",
    "!git clone https://github.com/gwinndr/MusicTransformer-Pytorch\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 2.3: QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mnt3p22/sp3_1\n",
      "total 8\n",
      "drwxrwxr-x 7 mnt3p22 mnt3p22 4096 Aug 29 16:32 MusicTransformer-Pytorch\n",
      "drwxrwxr-x 4 mnt3p22 mnt3p22 4096 Aug 29 16:29 midi-neural-processor\n",
      "Mon Aug 29 17:48:23 2022\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -ltA\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 3.1: Download pre-trained NN models  !!!!!!!!NONE!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘MusicTransformer-Pytorch/rpr’: File exists\n",
      "mkdir: cannot create directory ‘MusicTransformer-Pytorch/rpr/results’: File exists\n",
      "/home/mnt3p22/sp3_1/MusicTransformer-Pytorch/rpr/results\n",
      "--2022-08-30 09:31:59--  https://superpiano.s3-us-west-1.amazonaws.com/SuperPiano3models.zip\n",
      "Resolving superpiano.s3-us-west-1.amazonaws.com (superpiano.s3-us-west-1.amazonaws.com)... 52.219.113.41\n",
      "Connecting to superpiano.s3-us-west-1.amazonaws.com (superpiano.s3-us-west-1.amazonaws.com)|52.219.113.41|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2022-08-30 09:32:15 ERROR 404: Not Found.\n",
      "\n",
      "unzip:  cannot find or open SuperPiano3models.zip, SuperPiano3models.zip.zip or SuperPiano3models.zip.ZIP.\n",
      "[Errno 2] No such file or directory: 'MusicTransformer-Pytorch/'\n",
      "/home/mnt3p22/sp3_1/MusicTransformer-Pytorch/rpr/results\n",
      "Tue Aug 30 09:32:15 2022\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Pre-trained models download (2 models trained for 100 epochs to 1.968 FLoss and 0.420 acc)\n",
    "!mkdir MusicTransformer-Pytorch/rpr\n",
    "!mkdir MusicTransformer-Pytorch/rpr/results\n",
    "\n",
    "%cd MusicTransformer-Pytorch/rpr/results\n",
    "!wget 'https://superpiano.s3-us-west-1.amazonaws.com/SuperPiano3models.zip'\n",
    "!unzip SuperPiano3models.zip\n",
    "%cd MusicTransformer-Pytorch/\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "## 4.1: Download, process music dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05hD19W0hSCP"
   },
   "source": [
    "### 1.3: Setup Environment and Dependencies. Check GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "paYvoZHihtux"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'midi-neural-processor'...\n",
      "remote: Enumerating objects: 26, done.\u001b[K\n",
      "remote: Total 26 (delta 0), reused 0 (delta 0), pack-reused 26\u001b[K\n",
      "Unpacking objects: 100% (26/26), 7.99 KiB | 681.00 KiB/s, done.\n",
      "Cloning into 'MusicTransformer-Pytorch'...\n",
      "remote: Enumerating objects: 385, done.\u001b[K\n",
      "remote: Total 385 (delta 0), reused 0 (delta 0), pack-reused 385\u001b[K\n",
      "Receiving objects: 100% (385/385), 103.20 KiB | 3.33 MiB/s, done.\n",
      "Resolving deltas: 100% (206/206), done.\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (4.62.3)\n",
      "Collecting progress\n",
      "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
      "Building wheels for collected packages: progress\n",
      "  Building wheel for progress (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9615 sha256=f31afadd86c2328cdea2375e4bdf1bd59f3ab6e5dfcc70b445d7c14ecd51afed\n",
      "  Stored in directory: /home/mnt3p22/.cache/pip/wheels/bb/01/5a/c916509df9b12c6465864251dbe826def8e31a16fa7da54f08\n",
      "Successfully built progress\n",
      "Installing collected packages: progress\n",
      "Successfully installed progress-1.6\n",
      "Collecting pretty-midi\n",
      "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mido>=1.1.16\n",
      "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in ./.local/lib/python3.8/site-packages (from pretty-midi) (1.21.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pretty-midi) (1.14.0)\n",
      "Building wheels for collected packages: pretty-midi\n",
      "  Building wheel for pretty-midi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591952 sha256=938e79e69b6d7ed2293478ce05782559eff0405cc8ee13c44405b81a5376e3da\n",
      "  Stored in directory: /home/mnt3p22/.cache/pip/wheels/2a/5a/e3/30eeb9a99350f3f7e21258fcb132743eef1a4f49b3505e76b6\n",
      "Successfully built pretty-midi\n",
      "Installing collected packages: mido, pretty-midi\n",
      "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n",
      "Collecting pypianoroll\n",
      "  Downloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pretty-midi>=0.2.8 in ./.local/lib/python3.8/site-packages (from pypianoroll) (0.2.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.local/lib/python3.8/site-packages (from pypianoroll) (1.21.2)\n",
      "Requirement already satisfied: matplotlib>=1.5 in ./.local/lib/python3.8/site-packages (from pypianoroll) (3.5.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in ./.local/lib/python3.8/site-packages (from pypianoroll) (1.8.0)\n",
      "Requirement already satisfied: mido>=1.1.16 in ./.local/lib/python3.8/site-packages (from pretty-midi>=0.2.8->pypianoroll) (1.2.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pretty-midi>=0.2.8->pypianoroll) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.8/site-packages (from matplotlib>=1.5->pypianoroll) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib>=1.5->pypianoroll) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.8/site-packages (from matplotlib>=1.5->pypianoroll) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.8/site-packages (from matplotlib>=1.5->pypianoroll) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib>=1.5->pypianoroll) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./.local/lib/python3.8/site-packages (from matplotlib>=1.5->pypianoroll) (3.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib>=1.5->pypianoroll) (7.0.0)\n",
      "Installing collected packages: pypianoroll\n",
      "Successfully installed pypianoroll-1.0.4\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./.local/lib/python3.8/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 243 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in ./.local/lib/python3.8/site-packages (from librosa) (1.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: decorator>=4.0.10 in ./.local/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./.local/lib/python3.8/site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in ./.local/lib/python3.8/site-packages (from librosa) (1.21.2)\n",
      "Requirement already satisfied: joblib>=0.14 in ./.local/lib/python3.8/site-packages (from librosa) (1.1.0)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 820 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting audioread>=2.1.9\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soundfile>=0.10.2\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.45.1\n",
      "  Downloading numba-0.55.2-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.local/lib/python3.8/site-packages (from packaging>=20.0->librosa) (3.0.7)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.local/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from pooch>=1.0->librosa) (2.22.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in ./.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |███████▍                        | 7.9 MB 4.0 MB/s eta 0:00:07\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 425, in _error_catcher\n",
      "    yield\n",
      "  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/base_command.py\", line 186, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/commands/install.py\", line 357, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/legacy_resolve.py\", line 177, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/legacy_resolve.py\", line 333, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/legacy_resolve.py\", line 282, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 480, in prepare_linked_requirement\n",
      "    local_path = unpack_url(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 282, in unpack_url\n",
      "    return unpack_http_url(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 158, in unpack_http_url\n",
      "    from_path, content_type = _download_http_url(\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 303, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/utils/ui.py\", line 160, in iter\n",
      "    for x in it:\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_internal/network/utils.py\", line 15, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 529, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/usr/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 430, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in ./.local/lib/python3.8/site-packages (from scipy) (1.21.2)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (7.0.0)\n",
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
      "Collecting midi2audio\n",
      "  Downloading midi2audio-0.1.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: midi2audio\n",
      "Successfully installed midi2audio-0.1.1\n",
      "Collecting mir_eval\n",
      "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: future in /usr/lib/python3/dist-packages (from mir_eval) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in ./.local/lib/python3.8/site-packages (from mir_eval) (1.21.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in ./.local/lib/python3.8/site-packages (from mir_eval) (1.8.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from mir_eval) (1.14.0)\n",
      "Building wheels for collected packages: mir-eval\n",
      "  Building wheel for mir-eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100721 sha256=372d5a496a563f1ac169178457f3bf220da922cc17b22680e16f96e105226a83\n",
      "  Stored in directory: /home/mnt3p22/.cache/pip/wheels/20/53/83/1d50d15a666140d53eda589db005f7cb53b739c7e54711f51f\n",
      "Successfully built mir-eval\n",
      "Installing collected packages: mir-eval\n",
      "Successfully installed mir-eval-0.7\n",
      "cp: cannot stat '/usr/share/sounds/sf2/FluidR3_GM.sf2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#@title Clone/Install all dependencies\n",
    "!git clone https://github.com/asigalov61/midi-neural-processor\n",
    "!git clone https://github.com/asigalov61/MusicTransformer-Pytorch\n",
    "!pip install tqdm\n",
    "!pip install progress\n",
    "!pip install pretty-midi\n",
    "!pip install pypianoroll\n",
    "!pip install matplotlib\n",
    "!pip install librosa\n",
    "!pip install scipy\n",
    "!pip install pillow\n",
    "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
    "!pip install midi2audio\n",
    "!pip install mir_eval\n",
    "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 /content/font.sf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.2\n",
      "Tue Jul 26 15:01:04 2022\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#!pip install librosa\n",
    "\n",
    "# verify\n",
    "import librosa\n",
    "print(librosa.__version__)\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "VM71tUPVfffi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 26 15:01:05 2022\n"
     ]
    }
   ],
   "source": [
    "#@title Import all needed modules\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "# For plotting\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('SVG')\n",
    "#%matplotlib inline\n",
    "#matplotlib.get_backend()\n",
    "import mir_eval.display\n",
    "import librosa\n",
    "import librosa.display\n",
    "# For rendering output audio\n",
    "import pretty_midi\n",
    "from midi2audio import FluidSynth\n",
    "#from google.colab import output\n",
    "from IPython.display import display, Javascript, HTML, Audio\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "34spqHYPJtTJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mnt3p22/MusicTransformer-Pytorch/rpr/results\n",
      "--2022-07-26 15:02:43--  https://superpiano.s3-us-west-1.amazonaws.com/SuperPiano3models.zip\n",
      "Resolving superpiano.s3-us-west-1.amazonaws.com (superpiano.s3-us-west-1.amazonaws.com)... 52.219.192.82\n",
      "Connecting to superpiano.s3-us-west-1.amazonaws.com (superpiano.s3-us-west-1.amazonaws.com)|52.219.192.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2022-07-26 15:03:04 ERROR 404: Not Found.\n",
      "\n",
      "unzip:  cannot find or open SuperPiano3models.zip, SuperPiano3models.zip.zip or SuperPiano3models.zip.ZIP.\n",
      "/home/mnt3p22/MusicTransformer-Pytorch\n",
      "Tue Jul 26 15:03:04 2022\n"
     ]
    }
   ],
   "source": [
    "#@title (Optional) Pre-trained models download (2 models trained for 100 epochs to 1.968 FLoss and 0.420 acc)\n",
    "# mss\n",
    "#!mkdir /content/MusicTransformer-Pytorch/rpr\n",
    "#!mkdir /content/MusicTransformer-Pytorch/rpr/results\n",
    "#!mkdir -p /home/mnt3p22/MusicTransformer-Pytorch/rpr\n",
    "!mkdir -p /home/mnt3p22/MusicTransformer-Pytorch/rpr/results\n",
    "\n",
    "#%cd /content/MusicTransformer-Pytorch/rpr/results\n",
    "%cd /home/mnt3p22/MusicTransformer-Pytorch/rpr/results\n",
    "\n",
    "\n",
    "!wget 'https://superpiano.s3-us-west-1.amazonaws.com/SuperPiano3models.zip'\n",
    "!unzip SuperPiano3models.zip\n",
    "\n",
    "#%cd /content/MusicTransformer-Pytorch/\n",
    "%cd /home/mnt3p22/MusicTransformer-Pytorch/\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kE-VhygOPuG"
   },
   "source": [
    "#Please note that you MUST DOWNLOAD AND PROCESS ONE OF THE DATASETS TO TRAIN OR TO USE PRE-TRAINED MODEL as it primes the model from DATASET files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gd-O5LZyJGD"
   },
   "source": [
    "#Option 1: MAESTRO DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "0bGqw8o6oxUY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mnt3p22/MusicTransformer-Pytorch/dataset\n",
      "--2022-07-26 15:13:04--  https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... ^C\n",
      "Archive:  maestro-v2.0.0-midi.zip\n",
      "replace maestro-v2.0.0/maestro-v2.0.0.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "/home/mnt3p22/MusicTransformer-Pytorch\n",
      "Tue Jul 26 15:13:08 2022\n"
     ]
    }
   ],
   "source": [
    "#@title Download Google Magenta MAESTRO v.2.0.0 Piano MIDI Dataset (~1300 MIDIs)\n",
    "\n",
    "#%cd /content/MusicTransformer-Pytorch/dataset/\n",
    "%cd /home/mnt3p22/MusicTransformer-Pytorch/dataset/\n",
    "\n",
    "!wget 'https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip'\n",
    "!unzip maestro-v2.0.0-midi.zip\n",
    "\n",
    "#%cd /content/MusicTransformer-Pytorch/\n",
    "%cd /home/mnt3p22/MusicTransformer-Pytorch/\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "yXiyUuuonMqM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mnt3p22\n",
      "mv: cannot stat 'midi-neural-processor': No such file or directory\n",
      "/home/mnt3p22/MusicTransformer-Pytorch\n",
      "Tue Jul 26 15:13:17 2022\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare directory sctructure and MIDI processor\n",
    "#%cd /content/\n",
    "%cd /home/mnt3p22/\n",
    "\n",
    "!mv midi-neural-processor midi_processor\n",
    "#%cd /content/MusicTransformer-Pytorch/\n",
    "%cd /home/mnt3p22/MusicTransformer-Pytorch/\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "vN-bpkEGxSMY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing midi files and saving to ./dataset/e_piano\n",
      "Found 1282 pieces\n",
      "Preprocessing...\n",
      "50 / 1282\n",
      "100 / 1282\n",
      "150 / 1282\n",
      "200 / 1282\n",
      "250 / 1282\n",
      "300 / 1282\n",
      "350 / 1282\n",
      "400 / 1282\n",
      "450 / 1282\n",
      "500 / 1282\n",
      "550 / 1282\n",
      "600 / 1282\n",
      "650 / 1282\n",
      "700 / 1282\n",
      "750 / 1282\n",
      "800 / 1282\n",
      "850 / 1282\n",
      "900 / 1282\n",
      "950 / 1282\n",
      "1000 / 1282\n",
      "1050 / 1282\n",
      "1100 / 1282\n",
      "1150 / 1282\n",
      "1200 / 1282\n",
      "1250 / 1282\n",
      "Num Train: 967\n",
      "Num Val: 137\n",
      "Num Test: 178\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Process MAESTRO MIDI DataSet\n",
    "!python3 preprocess_midi.py '/home/mnt3p22/MusicTransformer-Pytorch/dataset/maestro-v2.0.0'\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKz4SKoeXYWc"
   },
   "source": [
    "#Option 2: Your own Custom MIDI DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "27zmkAM9vEGX"
   },
   "outputs": [],
   "source": [
    "#@title Create directory structure for the DataSet and prep MIDI processor\n",
    "\n",
    "\n",
    "\n",
    "#!mkdir '/content/MusicTransformer-Pytorch/dataset/e_piano/'\n",
    "#!mkdir '/content/MusicTransformer-Pytorch/dataset/e_piano/train'\n",
    "#!mkdir '/content/MusicTransformer-Pytorch/dataset/e_piano/test'\n",
    "#!mkdir '/content/MusicTransformer-Pytorch/dataset/e_piano/val'\n",
    "#!mkdir '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
    "\n",
    "!mkdir '/home/mnt3p22/MusicTransformer-Pytorch/dataset/e_piano/'\n",
    "!mkdir '/home/mnt3p22//MusicTransformer-Pytorch/dataset/e_piano/train'\n",
    "!mkdir '/home/mnt3p22/MusicTransformer-Pytorch/dataset/e_piano/test'\n",
    "!mkdir '/home/mnt3p22/MusicTransformer-Pytorch/dataset/e_piano/val'\n",
    "!mkdir '/home/mnt3p22/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
    "\n",
    "#%cd /content/\n",
    "%cd /home/mnt3p22/\n",
    "!mv midi-neural-processor midi_processor\n",
    "#%cd /content/MusicTransformer-Pytorch/\n",
    "%cd /home/mnt3p22/MusicTransformer-Pytorch/\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KUXaozztvGU2"
   },
   "outputs": [],
   "source": [
    "#@title Upload your custom MIDI DataSet to created \"dataset/e_piano/custom_midis\" folder through this cell or manually through any other means. You can also use ready-to-use DataSets below\n",
    "from google.colab import files\n",
    "%cd '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uDsmK-vkwOgl"
   },
   "outputs": [],
   "source": [
    "#@title (The Best Choice/Works best stand-alone) Super Piano 2 Original 2500 MIDIs of Piano Music\n",
    "%cd /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
    "!wget 'https://github.com/asigalov61/SuperPiano/raw/master/Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n",
    "!unzip -j 'Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n",
    "!rm Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "feA03YKvwgaV"
   },
   "outputs": [],
   "source": [
    "#@title (Second Best Choice/Works best stand-alone) Alex Piano Only Original 450 MIDIs \n",
    "%cd /content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis\n",
    "!wget 'https://github.com/asigalov61/AlexMIDIDataSet/raw/master/AlexMIDIDataSet-CC-BY-NC-SA-Piano-Only.zip'\n",
    "!unzip -j 'AlexMIDIDataSet-CC-BY-NC-SA-Piano-Only.zip'\n",
    "!rm AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-jAV_Qv5Fn_"
   },
   "source": [
    "For now, we are going to split the dataset by random into \"test\"/\"val\" dirs which is not ideal. So feel free to modify the code to your liking to achieve better training results with this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IZ5c6d5lXemo"
   },
   "outputs": [],
   "source": [
    "#@title Process your custom MIDI DataSet :)\n",
    "%cd /content/MusicTransformer-Pytorch\n",
    "from processor import encode_midi\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "%cd '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
    "\n",
    "custom_MIDI_DataSet_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/custom_midis'\n",
    "\n",
    "train_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/train' # split_type = 0\n",
    "test_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/test' # split_type = 1  \n",
    "val_dir = '/content/MusicTransformer-Pytorch/dataset/e_piano/val' # split_type = 2\n",
    "\n",
    "total_count = 0\n",
    "train_count = 0\n",
    "val_count   = 0\n",
    "test_count  = 0\n",
    "\n",
    "f_ext = '.pickle'\n",
    "fileList = os.listdir(custom_MIDI_DataSet_dir)\n",
    "for file in fileList:\n",
    "     # we gonna split by a random selection for now\n",
    "    \n",
    "    split = random.randint(1, 2)\n",
    "    if (split == 0):\n",
    "         o_file = os.path.join(train_dir, file+f_ext)\n",
    "         train_count += 1\n",
    "\n",
    "    elif (split == 2):\n",
    "         o_file0 = os.path.join(train_dir, file+f_ext)\n",
    "         train_count += 1\n",
    "         o_file = os.path.join(val_dir, file+f_ext)\n",
    "         val_count += 1\n",
    "\n",
    "    elif (split == 1):\n",
    "         o_file0 = os.path.join(train_dir, file+f_ext)\n",
    "         train_count += 1\n",
    "         o_file = os.path.join(test_dir, file+f_ext)\n",
    "         test_count += 1\n",
    "    try:\n",
    "      prepped = encode_midi(file)\n",
    "      o_stream = open(o_file0, \"wb\")\n",
    "      pickle.dump(prepped, o_stream)\n",
    "      o_stream.close()\n",
    "\n",
    "      prepped = encode_midi(file)\n",
    "      o_stream = open(o_file, \"wb\")\n",
    "      pickle.dump(prepped, o_stream)\n",
    "      o_stream.close()\n",
    "   \n",
    "      print(file)\n",
    "      print(o_file)\n",
    "      print('Coverted!')  \n",
    "    except KeyboardInterrupt: \n",
    "      raise   \n",
    "    except:\n",
    "      print('Bad file. Skipping...')\n",
    "\n",
    "print('Done')\n",
    "print(\"Num Train:\", train_count)\n",
    "print(\"Num Val:\", val_count)\n",
    "print(\"Num Test:\", test_count)\n",
    "print(\"Total Count:\", train_count)\n",
    "\n",
    "%cd /content/MusicTransformer-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwCQIziNwHxe"
   },
   "source": [
    "#Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "hwisXl2Iy_Xf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4619), started 0:00:17 ago. (Use '!kill 4619' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7425fa6abfdbefb9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7425fa6abfdbefb9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 26 15:23:00 2022\n"
     ]
    }
   ],
   "source": [
    "#@title Activate Tensorboard Graphs/Stats to monitor/evaluate model perfomance during and after training runs\n",
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "#%tensorboard --logdir /content/MusicTransformer-Pytorch/rpr\n",
    "%tensorboard --logdir /home/mnt3p22/MusicTransformer-Pytorch/rpr\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Sbv_sJyLq5om",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 26 15:44:17 2022\n",
      "=========================\n",
      "input_dir: ./dataset/e_piano\n",
      "output_dir: rpr\n",
      "weight_modulus: 1\n",
      "print_modulus: 1\n",
      "\n",
      "n_workers: 1\n",
      "force_cpu: False\n",
      "tensorboard: True\n",
      "\n",
      "continue_weights: None\n",
      "continue_epoch: None\n",
      "\n",
      "lr: None\n",
      "ce_smoothing: None\n",
      "batch_size: 4\n",
      "epochs: 50\n",
      "\n",
      "rpr: True\n",
      "max_sequence: 2048\n",
      "n_layers: 6\n",
      "num_heads: 8\n",
      "d_model: 512\n",
      "\n",
      "dim_feedforward: 1024\n",
      "dropout: 0.1\n",
      "=========================\n",
      "\n",
      "2022-07-26 15:44:18.596454: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-26 15:44:18.596487: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "=========================\n",
      "Baseline model evaluation (Epoch 0):\n",
      "Epoch: 0\n",
      "Avg train loss: 6.219924540559123\n",
      "Avg train acc: 0.002007490427094735\n",
      "Avg eval loss: 6.220743380652534\n",
      "Avg eval acc: 0.0018272959808301595\n",
      "=========================\n",
      "\n",
      "\n",
      "=========================\n",
      "NEW EPOCH: 1\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 1 / 242\n",
      "LR: 1.7469281074217108e-07\n",
      "Train loss: 6.2096123695373535\n",
      "\n",
      "Time (s): 0.5496776103973389\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 2 / 242\n",
      "LR: 3.4938562148434215e-07\n",
      "Train loss: 6.218203544616699\n",
      "\n",
      "Time (s): 0.49573588371276855\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 3 / 242\n",
      "LR: 5.240784322265132e-07\n",
      "Train loss: 6.165338039398193\n",
      "\n",
      "Time (s): 0.4895763397216797\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 4 / 242\n",
      "LR: 6.987712429686843e-07\n",
      "Train loss: 6.192784309387207\n",
      "\n",
      "Time (s): 0.4905433654785156\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 5 / 242\n",
      "LR: 8.734640537108554e-07\n",
      "Train loss: 6.211103439331055\n",
      "\n",
      "Time (s): 0.5031454563140869\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 6 / 242\n",
      "LR: 1.0481568644530265e-06\n",
      "Train loss: 6.16645622253418\n",
      "\n",
      "Time (s): 0.48934459686279297\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 7 / 242\n",
      "LR: 1.2228496751951975e-06\n",
      "Train loss: 6.191964626312256\n",
      "\n",
      "Time (s): 0.48310041427612305\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 8 / 242\n",
      "LR: 1.3975424859373686e-06\n",
      "Train loss: 6.197699069976807\n",
      "\n",
      "Time (s): 0.4875528812408447\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 9 / 242\n",
      "LR: 1.5722352966795397e-06\n",
      "Train loss: 6.178358554840088\n",
      "\n",
      "Time (s): 0.47490715980529785\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 10 / 242\n",
      "LR: 1.7469281074217108e-06\n",
      "Train loss: 6.1762776374816895\n",
      "\n",
      "Time (s): 0.4913959503173828\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 11 / 242\n",
      "LR: 1.9216209181638816e-06\n",
      "Train loss: 6.178898811340332\n",
      "\n",
      "Time (s): 0.47438573837280273\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 12 / 242\n",
      "LR: 2.096313728906053e-06\n",
      "Train loss: 6.153327941894531\n",
      "\n",
      "Time (s): 0.49672532081604004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 13 / 242\n",
      "LR: 2.271006539648224e-06\n",
      "Train loss: 6.149059772491455\n",
      "\n",
      "Time (s): 0.4838688373565674\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 14 / 242\n",
      "LR: 2.445699350390395e-06\n",
      "Train loss: 6.155942440032959\n",
      "\n",
      "Time (s): 0.4889681339263916\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 15 / 242\n",
      "LR: 2.620392161132566e-06\n",
      "Train loss: 6.12078332901001\n",
      "\n",
      "Time (s): 0.4734029769897461\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 16 / 242\n",
      "LR: 2.7950849718747372e-06\n",
      "Train loss: 6.112100601196289\n",
      "\n",
      "Time (s): 0.4901890754699707\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 17 / 242\n",
      "LR: 2.9697777826169085e-06\n",
      "Train loss: 6.108959674835205\n",
      "\n",
      "Time (s): 0.4817824363708496\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 18 / 242\n",
      "LR: 3.1444705933590794e-06\n",
      "Train loss: 6.076725482940674\n",
      "\n",
      "Time (s): 0.4789466857910156\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 19 / 242\n",
      "LR: 3.3191634041012502e-06\n",
      "Train loss: 6.0897746086120605\n",
      "\n",
      "Time (s): 0.4846935272216797\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 20 / 242\n",
      "LR: 3.4938562148434215e-06\n",
      "Train loss: 6.073618412017822\n",
      "\n",
      "Time (s): 0.4794039726257324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 21 / 242\n",
      "LR: 3.668549025585593e-06\n",
      "Train loss: 6.053652286529541\n",
      "\n",
      "Time (s): 0.47948265075683594\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 22 / 242\n",
      "LR: 3.843241836327763e-06\n",
      "Train loss: 6.046456813812256\n",
      "\n",
      "Time (s): 0.4784092903137207\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 23 / 242\n",
      "LR: 4.0179346470699345e-06\n",
      "Train loss: 6.003459930419922\n",
      "\n",
      "Time (s): 0.4812784194946289\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 24 / 242\n",
      "LR: 4.192627457812106e-06\n",
      "Train loss: 6.011580944061279\n",
      "\n",
      "Time (s): 0.48287248611450195\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 25 / 242\n",
      "LR: 4.367320268554277e-06\n",
      "Train loss: 5.986445426940918\n",
      "\n",
      "Time (s): 0.48414182662963867\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 26 / 242\n",
      "LR: 4.542013079296448e-06\n",
      "Train loss: 5.952127456665039\n",
      "\n",
      "Time (s): 0.4873976707458496\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 27 / 242\n",
      "LR: 4.716705890038619e-06\n",
      "Train loss: 5.947590351104736\n",
      "\n",
      "Time (s): 0.47589612007141113\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 28 / 242\n",
      "LR: 4.89139870078079e-06\n",
      "Train loss: 5.934291362762451\n",
      "\n",
      "Time (s): 0.48015737533569336\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 29 / 242\n",
      "LR: 5.066091511522961e-06\n",
      "Train loss: 5.911334037780762\n",
      "\n",
      "Time (s): 0.47898364067077637\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 30 / 242\n",
      "LR: 5.240784322265132e-06\n",
      "Train loss: 5.878510475158691\n",
      "\n",
      "Time (s): 0.490466833114624\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 31 / 242\n",
      "LR: 5.415477133007303e-06\n",
      "Train loss: 5.886538982391357\n",
      "\n",
      "Time (s): 0.4801464080810547\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 32 / 242\n",
      "LR: 5.5901699437494744e-06\n",
      "Train loss: 5.887195110321045\n",
      "\n",
      "Time (s): 0.5066201686859131\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 33 / 242\n",
      "LR: 5.764862754491646e-06\n",
      "Train loss: 5.84243106842041\n",
      "\n",
      "Time (s): 0.49628663063049316\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 34 / 242\n",
      "LR: 5.939555565233817e-06\n",
      "Train loss: 5.841352462768555\n",
      "\n",
      "Time (s): 0.49957847595214844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 35 / 242\n",
      "LR: 6.1142483759759874e-06\n",
      "Train loss: 5.783762454986572\n",
      "\n",
      "Time (s): 0.4824802875518799\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 36 / 242\n",
      "LR: 6.288941186718159e-06\n",
      "Train loss: 5.809736251831055\n",
      "\n",
      "Time (s): 0.4814596176147461\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 37 / 242\n",
      "LR: 6.46363399746033e-06\n",
      "Train loss: 5.771295547485352\n",
      "\n",
      "Time (s): 0.4799766540527344\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 38 / 242\n",
      "LR: 6.6383268082025005e-06\n",
      "Train loss: 5.755420207977295\n",
      "\n",
      "Time (s): 0.4799003601074219\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 39 / 242\n",
      "LR: 6.813019618944672e-06\n",
      "Train loss: 5.726345539093018\n",
      "\n",
      "Time (s): 0.48320579528808594\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 40 / 242\n",
      "LR: 6.987712429686843e-06\n",
      "Train loss: 5.730118751525879\n",
      "\n",
      "Time (s): 0.48768138885498047\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 41 / 242\n",
      "LR: 7.162405240429014e-06\n",
      "Train loss: 5.681617736816406\n",
      "\n",
      "Time (s): 0.48290514945983887\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 42 / 242\n",
      "LR: 7.337098051171186e-06\n",
      "Train loss: 5.7322587966918945\n",
      "\n",
      "Time (s): 0.5064451694488525\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 43 / 242\n",
      "LR: 7.511790861913356e-06\n",
      "Train loss: 5.689924716949463\n",
      "\n",
      "Time (s): 0.48079562187194824\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 44 / 242\n",
      "LR: 7.686483672655526e-06\n",
      "Train loss: 5.649189472198486\n",
      "\n",
      "Time (s): 0.48181939125061035\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 45 / 242\n",
      "LR: 7.861176483397698e-06\n",
      "Train loss: 5.633974552154541\n",
      "\n",
      "Time (s): 0.4848012924194336\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 46 / 242\n",
      "LR: 8.035869294139869e-06\n",
      "Train loss: 5.568641185760498\n",
      "\n",
      "Time (s): 0.4844973087310791\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 47 / 242\n",
      "LR: 8.21056210488204e-06\n",
      "Train loss: 5.6274518966674805\n",
      "\n",
      "Time (s): 0.4848337173461914\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 48 / 242\n",
      "LR: 8.385254915624212e-06\n",
      "Train loss: 5.628151893615723\n",
      "\n",
      "Time (s): 0.48672938346862793\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 49 / 242\n",
      "LR: 8.559947726366383e-06\n",
      "Train loss: 5.629975318908691\n",
      "\n",
      "Time (s): 0.4768249988555908\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 50 / 242\n",
      "LR: 8.734640537108554e-06\n",
      "Train loss: 5.577561855316162\n",
      "\n",
      "Time (s): 0.479459285736084\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 51 / 242\n",
      "LR: 8.909333347850726e-06\n",
      "Train loss: 5.602370738983154\n",
      "\n",
      "Time (s): 0.4835672378540039\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 52 / 242\n",
      "LR: 9.084026158592897e-06\n",
      "Train loss: 5.605460166931152\n",
      "\n",
      "Time (s): 0.4733145236968994\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 53 / 242\n",
      "LR: 9.258718969335066e-06\n",
      "Train loss: 5.535696983337402\n",
      "\n",
      "Time (s): 0.4741337299346924\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 54 / 242\n",
      "LR: 9.433411780077238e-06\n",
      "Train loss: 5.523232936859131\n",
      "\n",
      "Time (s): 0.4907076358795166\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 55 / 242\n",
      "LR: 9.608104590819409e-06\n",
      "Train loss: 5.537463665008545\n",
      "\n",
      "Time (s): 0.4770011901855469\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 56 / 242\n",
      "LR: 9.78279740156158e-06\n",
      "Train loss: 5.518753528594971\n",
      "\n",
      "Time (s): 0.49818944931030273\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 57 / 242\n",
      "LR: 9.957490212303752e-06\n",
      "Train loss: 5.401818752288818\n",
      "\n",
      "Time (s): 0.4744143486022949\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 58 / 242\n",
      "LR: 1.0132183023045923e-05\n",
      "Train loss: 5.511133670806885\n",
      "\n",
      "Time (s): 0.4662740230560303\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 59 / 242\n",
      "LR: 1.0306875833788094e-05\n",
      "Train loss: 5.51506233215332\n",
      "\n",
      "Time (s): 0.4768638610839844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 60 / 242\n",
      "LR: 1.0481568644530264e-05\n",
      "Train loss: 5.486952304840088\n",
      "\n",
      "Time (s): 0.4764676094055176\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 61 / 242\n",
      "LR: 1.0656261455272435e-05\n",
      "Train loss: 5.414024829864502\n",
      "\n",
      "Time (s): 0.4686133861541748\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 62 / 242\n",
      "LR: 1.0830954266014606e-05\n",
      "Train loss: 5.481808185577393\n",
      "\n",
      "Time (s): 0.473421573638916\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 63 / 242\n",
      "LR: 1.1005647076756778e-05\n",
      "Train loss: 5.439353942871094\n",
      "\n",
      "Time (s): 0.4706151485443115\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 64 / 242\n",
      "LR: 1.1180339887498949e-05\n",
      "Train loss: 5.42539119720459\n",
      "\n",
      "Time (s): 0.4783923625946045\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 65 / 242\n",
      "LR: 1.135503269824112e-05\n",
      "Train loss: 5.385034561157227\n",
      "\n",
      "Time (s): 0.47220468521118164\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 66 / 242\n",
      "LR: 1.1529725508983291e-05\n",
      "Train loss: 5.395717620849609\n",
      "\n",
      "Time (s): 0.47896742820739746\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 67 / 242\n",
      "LR: 1.1704418319725463e-05\n",
      "Train loss: 5.531466484069824\n",
      "\n",
      "Time (s): 0.5080645084381104\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 68 / 242\n",
      "LR: 1.1879111130467634e-05\n",
      "Train loss: 5.381711483001709\n",
      "\n",
      "Time (s): 0.4952254295349121\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 69 / 242\n",
      "LR: 1.2053803941209804e-05\n",
      "Train loss: 5.374159812927246\n",
      "\n",
      "Time (s): 0.489316463470459\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 70 / 242\n",
      "LR: 1.2228496751951975e-05\n",
      "Train loss: 5.303962230682373\n",
      "\n",
      "Time (s): 0.4895341396331787\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 71 / 242\n",
      "LR: 1.2403189562694146e-05\n",
      "Train loss: 5.40424108505249\n",
      "\n",
      "Time (s): 0.48809337615966797\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 72 / 242\n",
      "LR: 1.2577882373436317e-05\n",
      "Train loss: 5.297304153442383\n",
      "\n",
      "Time (s): 0.49007248878479004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 73 / 242\n",
      "LR: 1.2752575184178489e-05\n",
      "Train loss: 5.371078968048096\n",
      "\n",
      "Time (s): 0.4841763973236084\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 74 / 242\n",
      "LR: 1.292726799492066e-05\n",
      "Train loss: 5.353146076202393\n",
      "\n",
      "Time (s): 0.4858829975128174\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 75 / 242\n",
      "LR: 1.3101960805662831e-05\n",
      "Train loss: 5.353289604187012\n",
      "\n",
      "Time (s): 0.4863407611846924\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 76 / 242\n",
      "LR: 1.3276653616405001e-05\n",
      "Train loss: 5.360177040100098\n",
      "\n",
      "Time (s): 0.48952436447143555\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 77 / 242\n",
      "LR: 1.3451346427147172e-05\n",
      "Train loss: 5.299210548400879\n",
      "\n",
      "Time (s): 0.48861050605773926\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 78 / 242\n",
      "LR: 1.3626039237889344e-05\n",
      "Train loss: 5.283282279968262\n",
      "\n",
      "Time (s): 0.510228157043457\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 79 / 242\n",
      "LR: 1.3800732048631515e-05\n",
      "Train loss: 5.29746150970459\n",
      "\n",
      "Time (s): 0.4964625835418701\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 80 / 242\n",
      "LR: 1.3975424859373686e-05\n",
      "Train loss: 5.27159309387207\n",
      "\n",
      "Time (s): 0.49027204513549805\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 81 / 242\n",
      "LR: 1.4150117670115857e-05\n",
      "Train loss: 5.303609371185303\n",
      "\n",
      "Time (s): 0.5076894760131836\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 82 / 242\n",
      "LR: 1.4324810480858029e-05\n",
      "Train loss: 5.2350897789001465\n",
      "\n",
      "Time (s): 0.4945862293243408\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 83 / 242\n",
      "LR: 1.44995032916002e-05\n",
      "Train loss: 5.232152938842773\n",
      "\n",
      "Time (s): 0.5081195831298828\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 84 / 242\n",
      "LR: 1.4674196102342371e-05\n",
      "Train loss: 5.243727207183838\n",
      "\n",
      "Time (s): 0.5079476833343506\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 85 / 242\n",
      "LR: 1.484888891308454e-05\n",
      "Train loss: 5.203001499176025\n",
      "\n",
      "Time (s): 0.506094217300415\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 86 / 242\n",
      "LR: 1.5023581723826712e-05\n",
      "Train loss: 5.278648853302002\n",
      "\n",
      "Time (s): 0.4892561435699463\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 87 / 242\n",
      "LR: 1.5198274534568883e-05\n",
      "Train loss: 5.285680294036865\n",
      "\n",
      "Time (s): 0.48715996742248535\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 88 / 242\n",
      "LR: 1.5372967345311053e-05\n",
      "Train loss: 5.247414588928223\n",
      "\n",
      "Time (s): 0.48613405227661133\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 89 / 242\n",
      "LR: 1.5547660156053224e-05\n",
      "Train loss: 5.225863933563232\n",
      "\n",
      "Time (s): 0.4999117851257324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 90 / 242\n",
      "LR: 1.5722352966795396e-05\n",
      "Train loss: 5.222820281982422\n",
      "\n",
      "Time (s): 0.4894721508026123\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 91 / 242\n",
      "LR: 1.5897045777537567e-05\n",
      "Train loss: 5.277093887329102\n",
      "\n",
      "Time (s): 0.4852719306945801\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 92 / 242\n",
      "LR: 1.6071738588279738e-05\n",
      "Train loss: 5.252242565155029\n",
      "\n",
      "Time (s): 0.5081131458282471\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 93 / 242\n",
      "LR: 1.624643139902191e-05\n",
      "Train loss: 5.17939567565918\n",
      "\n",
      "Time (s): 0.506810188293457\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 94 / 242\n",
      "LR: 1.642112420976408e-05\n",
      "Train loss: 5.3357768058776855\n",
      "\n",
      "Time (s): 0.4821789264678955\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 95 / 242\n",
      "LR: 1.6595817020506252e-05\n",
      "Train loss: 5.211167812347412\n",
      "\n",
      "Time (s): 0.4846799373626709\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 96 / 242\n",
      "LR: 1.6770509831248423e-05\n",
      "Train loss: 5.211236953735352\n",
      "\n",
      "Time (s): 0.4847695827484131\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 97 / 242\n",
      "LR: 1.6945202641990595e-05\n",
      "Train loss: 5.154540061950684\n",
      "\n",
      "Time (s): 0.5116913318634033\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 98 / 242\n",
      "LR: 1.7119895452732766e-05\n",
      "Train loss: 5.267109394073486\n",
      "\n",
      "Time (s): 0.48053765296936035\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 99 / 242\n",
      "LR: 1.7294588263474937e-05\n",
      "Train loss: 5.227004051208496\n",
      "\n",
      "Time (s): 0.485612154006958\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 100 / 242\n",
      "LR: 1.746928107421711e-05\n",
      "Train loss: 5.192378520965576\n",
      "\n",
      "Time (s): 0.4844965934753418\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 101 / 242\n",
      "LR: 1.764397388495928e-05\n",
      "Train loss: 5.158049583435059\n",
      "\n",
      "Time (s): 0.4824371337890625\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 102 / 242\n",
      "LR: 1.781866669570145e-05\n",
      "Train loss: 5.172916412353516\n",
      "\n",
      "Time (s): 0.48577070236206055\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 103 / 242\n",
      "LR: 1.7993359506443622e-05\n",
      "Train loss: 5.307127952575684\n",
      "\n",
      "Time (s): 0.4989054203033447\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 104 / 242\n",
      "LR: 1.8168052317185794e-05\n",
      "Train loss: 5.159178256988525\n",
      "\n",
      "Time (s): 0.48667144775390625\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 105 / 242\n",
      "LR: 1.834274512792796e-05\n",
      "Train loss: 5.135143756866455\n",
      "\n",
      "Time (s): 0.4889411926269531\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 106 / 242\n",
      "LR: 1.8517437938670133e-05\n",
      "Train loss: 5.1296796798706055\n",
      "\n",
      "Time (s): 0.48815202713012695\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 107 / 242\n",
      "LR: 1.8692130749412304e-05\n",
      "Train loss: 5.137300491333008\n",
      "\n",
      "Time (s): 0.484055757522583\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 108 / 242\n",
      "LR: 1.8866823560154475e-05\n",
      "Train loss: 5.164741039276123\n",
      "\n",
      "Time (s): 0.48722004890441895\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 109 / 242\n",
      "LR: 1.9041516370896647e-05\n",
      "Train loss: 5.125057220458984\n",
      "\n",
      "Time (s): 0.5093793869018555\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 110 / 242\n",
      "LR: 1.9216209181638818e-05\n",
      "Train loss: 5.145739555358887\n",
      "\n",
      "Time (s): 0.5107929706573486\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 111 / 242\n",
      "LR: 1.939090199238099e-05\n",
      "Train loss: 5.2196478843688965\n",
      "\n",
      "Time (s): 0.48537421226501465\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 112 / 242\n",
      "LR: 1.956559480312316e-05\n",
      "Train loss: 5.166316986083984\n",
      "\n",
      "Time (s): 0.4786646366119385\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 113 / 242\n",
      "LR: 1.9740287613865332e-05\n",
      "Train loss: 5.124920845031738\n",
      "\n",
      "Time (s): 0.48607397079467773\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 114 / 242\n",
      "LR: 1.9914980424607503e-05\n",
      "Train loss: 5.094542026519775\n",
      "\n",
      "Time (s): 0.4869098663330078\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 115 / 242\n",
      "LR: 2.0089673235349674e-05\n",
      "Train loss: 5.179373264312744\n",
      "\n",
      "Time (s): 0.4879777431488037\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 116 / 242\n",
      "LR: 2.0264366046091846e-05\n",
      "Train loss: 5.049836158752441\n",
      "\n",
      "Time (s): 0.4850449562072754\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 117 / 242\n",
      "LR: 2.0439058856834017e-05\n",
      "Train loss: 5.0827813148498535\n",
      "\n",
      "Time (s): 0.48708152770996094\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 118 / 242\n",
      "LR: 2.0613751667576188e-05\n",
      "Train loss: 5.115497589111328\n",
      "\n",
      "Time (s): 0.49002504348754883\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 119 / 242\n",
      "LR: 2.078844447831836e-05\n",
      "Train loss: 5.1049699783325195\n",
      "\n",
      "Time (s): 0.4881882667541504\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 120 / 242\n",
      "LR: 2.0963137289060527e-05\n",
      "Train loss: 5.121859073638916\n",
      "\n",
      "Time (s): 0.48058319091796875\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 121 / 242\n",
      "LR: 2.11378300998027e-05\n",
      "Train loss: 5.128348350524902\n",
      "\n",
      "Time (s): 0.4816927909851074\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 122 / 242\n",
      "LR: 2.131252291054487e-05\n",
      "Train loss: 5.12130880355835\n",
      "\n",
      "Time (s): 0.49036240577697754\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 123 / 242\n",
      "LR: 2.148721572128704e-05\n",
      "Train loss: 5.137230396270752\n",
      "\n",
      "Time (s): 0.48061513900756836\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 124 / 242\n",
      "LR: 2.1661908532029213e-05\n",
      "Train loss: 5.202988147735596\n",
      "\n",
      "Time (s): 0.475067138671875\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 125 / 242\n",
      "LR: 2.1836601342771384e-05\n",
      "Train loss: 5.2023844718933105\n",
      "\n",
      "Time (s): 0.47713136672973633\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 126 / 242\n",
      "LR: 2.2011294153513555e-05\n",
      "Train loss: 5.061723709106445\n",
      "\n",
      "Time (s): 0.48290228843688965\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 127 / 242\n",
      "LR: 2.2185986964255726e-05\n",
      "Train loss: 5.094452857971191\n",
      "\n",
      "Time (s): 0.4826838970184326\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 128 / 242\n",
      "LR: 2.2360679774997898e-05\n",
      "Train loss: 5.06679105758667\n",
      "\n",
      "Time (s): 0.4758453369140625\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 129 / 242\n",
      "LR: 2.253537258574007e-05\n",
      "Train loss: 5.039280414581299\n",
      "\n",
      "Time (s): 0.48903822898864746\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 130 / 242\n",
      "LR: 2.271006539648224e-05\n",
      "Train loss: 4.96410608291626\n",
      "\n",
      "Time (s): 0.49820947647094727\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 131 / 242\n",
      "LR: 2.288475820722441e-05\n",
      "Train loss: 5.1072096824646\n",
      "\n",
      "Time (s): 0.49626898765563965\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 132 / 242\n",
      "LR: 2.3059451017966583e-05\n",
      "Train loss: 5.044765949249268\n",
      "\n",
      "Time (s): 0.5091726779937744\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 133 / 242\n",
      "LR: 2.3234143828708754e-05\n",
      "Train loss: 5.076617240905762\n",
      "\n",
      "Time (s): 0.4852614402770996\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 134 / 242\n",
      "LR: 2.3408836639450925e-05\n",
      "Train loss: 5.133541584014893\n",
      "\n",
      "Time (s): 0.5085089206695557\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 135 / 242\n",
      "LR: 2.3583529450193097e-05\n",
      "Train loss: 5.041702747344971\n",
      "\n",
      "Time (s): 0.5059773921966553\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 136 / 242\n",
      "LR: 2.3758222260935268e-05\n",
      "Train loss: 5.030649185180664\n",
      "\n",
      "Time (s): 0.49540257453918457\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 137 / 242\n",
      "LR: 2.3932915071677436e-05\n",
      "Train loss: 4.970940589904785\n",
      "\n",
      "Time (s): 0.4827103614807129\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 138 / 242\n",
      "LR: 2.4107607882419607e-05\n",
      "Train loss: 5.170495986938477\n",
      "\n",
      "Time (s): 0.5022284984588623\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 139 / 242\n",
      "LR: 2.428230069316178e-05\n",
      "Train loss: 5.0117506980896\n",
      "\n",
      "Time (s): 0.48760104179382324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 140 / 242\n",
      "LR: 2.445699350390395e-05\n",
      "Train loss: 5.097647666931152\n",
      "\n",
      "Time (s): 0.5033891201019287\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 141 / 242\n",
      "LR: 2.463168631464612e-05\n",
      "Train loss: 5.180253028869629\n",
      "\n",
      "Time (s): 0.5123968124389648\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 142 / 242\n",
      "LR: 2.4806379125388292e-05\n",
      "Train loss: 5.107125759124756\n",
      "\n",
      "Time (s): 0.48896026611328125\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 143 / 242\n",
      "LR: 2.4981071936130464e-05\n",
      "Train loss: 5.086872100830078\n",
      "\n",
      "Time (s): 0.4856240749359131\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 144 / 242\n",
      "LR: 2.5155764746872635e-05\n",
      "Train loss: 5.036364555358887\n",
      "\n",
      "Time (s): 0.5392558574676514\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 145 / 242\n",
      "LR: 2.5330457557614806e-05\n",
      "Train loss: 4.969360828399658\n",
      "\n",
      "Time (s): 0.525482177734375\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 146 / 242\n",
      "LR: 2.5505150368356978e-05\n",
      "Train loss: 5.1619720458984375\n",
      "\n",
      "Time (s): 0.5044076442718506\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 147 / 242\n",
      "LR: 2.567984317909915e-05\n",
      "Train loss: 4.9755473136901855\n",
      "\n",
      "Time (s): 0.5197093486785889\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 148 / 242\n",
      "LR: 2.585453598984132e-05\n",
      "Train loss: 5.058281898498535\n",
      "\n",
      "Time (s): 0.5025568008422852\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 149 / 242\n",
      "LR: 2.602922880058349e-05\n",
      "Train loss: 4.980853080749512\n",
      "\n",
      "Time (s): 0.5160069465637207\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 150 / 242\n",
      "LR: 2.6203921611325663e-05\n",
      "Train loss: 4.927911281585693\n",
      "\n",
      "Time (s): 0.5460684299468994\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 151 / 242\n",
      "LR: 2.6378614422067834e-05\n",
      "Train loss: 4.885988712310791\n",
      "\n",
      "Time (s): 0.5247173309326172\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 152 / 242\n",
      "LR: 2.6553307232810002e-05\n",
      "Train loss: 5.107227325439453\n",
      "\n",
      "Time (s): 0.5104293823242188\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 153 / 242\n",
      "LR: 2.6728000043552173e-05\n",
      "Train loss: 5.002608776092529\n",
      "\n",
      "Time (s): 0.49033284187316895\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 154 / 242\n",
      "LR: 2.6902692854294344e-05\n",
      "Train loss: 4.995660781860352\n",
      "\n",
      "Time (s): 0.5241379737854004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 155 / 242\n",
      "LR: 2.7077385665036516e-05\n",
      "Train loss: 5.050589561462402\n",
      "\n",
      "Time (s): 0.5090961456298828\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 156 / 242\n",
      "LR: 2.7252078475778687e-05\n",
      "Train loss: 4.995239734649658\n",
      "\n",
      "Time (s): 0.497314453125\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 157 / 242\n",
      "LR: 2.7426771286520858e-05\n",
      "Train loss: 4.966523170471191\n",
      "\n",
      "Time (s): 0.52740478515625\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 158 / 242\n",
      "LR: 2.760146409726303e-05\n",
      "Train loss: 4.945976257324219\n",
      "\n",
      "Time (s): 0.5021243095397949\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 159 / 242\n",
      "LR: 2.77761569080052e-05\n",
      "Train loss: 5.157657623291016\n",
      "\n",
      "Time (s): 0.48531508445739746\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 160 / 242\n",
      "LR: 2.7950849718747372e-05\n",
      "Train loss: 4.929046154022217\n",
      "\n",
      "Time (s): 0.48410987854003906\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 161 / 242\n",
      "LR: 2.8125542529489543e-05\n",
      "Train loss: 5.047713279724121\n",
      "\n",
      "Time (s): 0.489016056060791\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 162 / 242\n",
      "LR: 2.8300235340231715e-05\n",
      "Train loss: 4.932480335235596\n",
      "\n",
      "Time (s): 0.4888768196105957\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 163 / 242\n",
      "LR: 2.8474928150973886e-05\n",
      "Train loss: 5.029138565063477\n",
      "\n",
      "Time (s): 0.49234843254089355\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 164 / 242\n",
      "LR: 2.8649620961716057e-05\n",
      "Train loss: 4.931600570678711\n",
      "\n",
      "Time (s): 0.4825479984283447\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 165 / 242\n",
      "LR: 2.882431377245823e-05\n",
      "Train loss: 5.010904788970947\n",
      "\n",
      "Time (s): 0.5009548664093018\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 166 / 242\n",
      "LR: 2.89990065832004e-05\n",
      "Train loss: 4.910342693328857\n",
      "\n",
      "Time (s): 0.4952845573425293\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 167 / 242\n",
      "LR: 2.917369939394257e-05\n",
      "Train loss: 4.922118663787842\n",
      "\n",
      "Time (s): 0.5237390995025635\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 168 / 242\n",
      "LR: 2.9348392204684742e-05\n",
      "Train loss: 4.988205432891846\n",
      "\n",
      "Time (s): 0.5808804035186768\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 169 / 242\n",
      "LR: 2.952308501542691e-05\n",
      "Train loss: 4.887089252471924\n",
      "\n",
      "Time (s): 0.5222370624542236\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 170 / 242\n",
      "LR: 2.969777782616908e-05\n",
      "Train loss: 4.892804145812988\n",
      "\n",
      "Time (s): 0.5215699672698975\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 171 / 242\n",
      "LR: 2.9872470636911253e-05\n",
      "Train loss: 4.992526054382324\n",
      "\n",
      "Time (s): 0.5301871299743652\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 172 / 242\n",
      "LR: 3.0047163447653424e-05\n",
      "Train loss: 4.978570938110352\n",
      "\n",
      "Time (s): 0.5042173862457275\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 173 / 242\n",
      "LR: 3.0221856258395596e-05\n",
      "Train loss: 4.988797187805176\n",
      "\n",
      "Time (s): 0.5002892017364502\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 174 / 242\n",
      "LR: 3.0396549069137767e-05\n",
      "Train loss: 4.892658710479736\n",
      "\n",
      "Time (s): 0.5245101451873779\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 175 / 242\n",
      "LR: 3.057124187987994e-05\n",
      "Train loss: 5.070443630218506\n",
      "\n",
      "Time (s): 0.4985697269439697\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 176 / 242\n",
      "LR: 3.0745934690622106e-05\n",
      "Train loss: 5.052188873291016\n",
      "\n",
      "Time (s): 0.4872732162475586\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 177 / 242\n",
      "LR: 3.092062750136428e-05\n",
      "Train loss: 5.06035041809082\n",
      "\n",
      "Time (s): 0.474470853805542\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 178 / 242\n",
      "LR: 3.109532031210645e-05\n",
      "Train loss: 4.983834743499756\n",
      "\n",
      "Time (s): 0.5064167976379395\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 179 / 242\n",
      "LR: 3.127001312284862e-05\n",
      "Train loss: 4.99429988861084\n",
      "\n",
      "Time (s): 0.5251641273498535\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 180 / 242\n",
      "LR: 3.144470593359079e-05\n",
      "Train loss: 4.987434387207031\n",
      "\n",
      "Time (s): 0.49108242988586426\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 181 / 242\n",
      "LR: 3.1619398744332966e-05\n",
      "Train loss: 5.044334888458252\n",
      "\n",
      "Time (s): 0.48843860626220703\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 182 / 242\n",
      "LR: 3.1794091555075134e-05\n",
      "Train loss: 4.975882053375244\n",
      "\n",
      "Time (s): 0.4871659278869629\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 183 / 242\n",
      "LR: 3.196878436581731e-05\n",
      "Train loss: 4.932674884796143\n",
      "\n",
      "Time (s): 0.47600388526916504\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 184 / 242\n",
      "LR: 3.2143477176559476e-05\n",
      "Train loss: 5.059082508087158\n",
      "\n",
      "Time (s): 0.4732542037963867\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 185 / 242\n",
      "LR: 3.231816998730165e-05\n",
      "Train loss: 5.157790184020996\n",
      "\n",
      "Time (s): 0.4771914482116699\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 186 / 242\n",
      "LR: 3.249286279804382e-05\n",
      "Train loss: 4.855105400085449\n",
      "\n",
      "Time (s): 0.4757518768310547\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 187 / 242\n",
      "LR: 3.2667555608785994e-05\n",
      "Train loss: 4.8787102699279785\n",
      "\n",
      "Time (s): 0.4749295711517334\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 188 / 242\n",
      "LR: 3.284224841952816e-05\n",
      "Train loss: 4.933255672454834\n",
      "\n",
      "Time (s): 0.48026108741760254\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 189 / 242\n",
      "LR: 3.3016941230270336e-05\n",
      "Train loss: 4.984895706176758\n",
      "\n",
      "Time (s): 0.5109713077545166\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 190 / 242\n",
      "LR: 3.3191634041012504e-05\n",
      "Train loss: 4.8970046043396\n",
      "\n",
      "Time (s): 0.47861719131469727\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 191 / 242\n",
      "LR: 3.336632685175468e-05\n",
      "Train loss: 4.931707382202148\n",
      "\n",
      "Time (s): 0.47757887840270996\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 192 / 242\n",
      "LR: 3.3541019662496847e-05\n",
      "Train loss: 4.912495136260986\n",
      "\n",
      "Time (s): 0.47519373893737793\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 193 / 242\n",
      "LR: 3.3715712473239014e-05\n",
      "Train loss: 4.9584197998046875\n",
      "\n",
      "Time (s): 0.47870731353759766\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 194 / 242\n",
      "LR: 3.389040528398119e-05\n",
      "Train loss: 4.910014629364014\n",
      "\n",
      "Time (s): 0.46813154220581055\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 195 / 242\n",
      "LR: 3.406509809472336e-05\n",
      "Train loss: 4.884452819824219\n",
      "\n",
      "Time (s): 0.47350049018859863\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 196 / 242\n",
      "LR: 3.423979090546553e-05\n",
      "Train loss: 4.821633338928223\n",
      "\n",
      "Time (s): 0.46833252906799316\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 197 / 242\n",
      "LR: 3.44144837162077e-05\n",
      "Train loss: 4.912749767303467\n",
      "\n",
      "Time (s): 0.46984148025512695\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 198 / 242\n",
      "LR: 3.4589176526949874e-05\n",
      "Train loss: 4.844397068023682\n",
      "\n",
      "Time (s): 0.46718478202819824\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 199 / 242\n",
      "LR: 3.476386933769204e-05\n",
      "Train loss: 4.976861953735352\n",
      "\n",
      "Time (s): 0.5083024501800537\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 200 / 242\n",
      "LR: 3.493856214843422e-05\n",
      "Train loss: 4.9240217208862305\n",
      "\n",
      "Time (s): 0.4771885871887207\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 201 / 242\n",
      "LR: 3.5113254959176385e-05\n",
      "Train loss: 4.765621662139893\n",
      "\n",
      "Time (s): 0.49261903762817383\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 202 / 242\n",
      "LR: 3.528794776991856e-05\n",
      "Train loss: 4.815924644470215\n",
      "\n",
      "Time (s): 0.489696741104126\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 203 / 242\n",
      "LR: 3.546264058066073e-05\n",
      "Train loss: 4.891578197479248\n",
      "\n",
      "Time (s): 0.48660874366760254\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 204 / 242\n",
      "LR: 3.56373333914029e-05\n",
      "Train loss: 4.692265033721924\n",
      "\n",
      "Time (s): 0.48909735679626465\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 205 / 242\n",
      "LR: 3.581202620214507e-05\n",
      "Train loss: 5.01309061050415\n",
      "\n",
      "Time (s): 0.47569727897644043\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 206 / 242\n",
      "LR: 3.5986719012887245e-05\n",
      "Train loss: 4.778383255004883\n",
      "\n",
      "Time (s): 0.4815959930419922\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 207 / 242\n",
      "LR: 3.616141182362941e-05\n",
      "Train loss: 4.906373023986816\n",
      "\n",
      "Time (s): 0.48775720596313477\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 208 / 242\n",
      "LR: 3.633610463437159e-05\n",
      "Train loss: 4.969107151031494\n",
      "\n",
      "Time (s): 0.4964125156402588\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 209 / 242\n",
      "LR: 3.6510797445113755e-05\n",
      "Train loss: 4.864665508270264\n",
      "\n",
      "Time (s): 0.49501943588256836\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 210 / 242\n",
      "LR: 3.668549025585592e-05\n",
      "Train loss: 4.800525188446045\n",
      "\n",
      "Time (s): 0.5067746639251709\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 211 / 242\n",
      "LR: 3.68601830665981e-05\n",
      "Train loss: 4.872853755950928\n",
      "\n",
      "Time (s): 0.4823589324951172\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 212 / 242\n",
      "LR: 3.7034875877340266e-05\n",
      "Train loss: 4.797629356384277\n",
      "\n",
      "Time (s): 0.47445082664489746\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 213 / 242\n",
      "LR: 3.720956868808244e-05\n",
      "Train loss: 4.812355995178223\n",
      "\n",
      "Time (s): 0.4844965934753418\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 214 / 242\n",
      "LR: 3.738426149882461e-05\n",
      "Train loss: 4.832756042480469\n",
      "\n",
      "Time (s): 0.4836392402648926\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 215 / 242\n",
      "LR: 3.755895430956678e-05\n",
      "Train loss: 4.657774448394775\n",
      "\n",
      "Time (s): 0.4842183589935303\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 216 / 242\n",
      "LR: 3.773364712030895e-05\n",
      "Train loss: 4.766644477844238\n",
      "\n",
      "Time (s): 0.4841930866241455\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 217 / 242\n",
      "LR: 3.7908339931051125e-05\n",
      "Train loss: 4.787876605987549\n",
      "\n",
      "Time (s): 0.47653746604919434\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 218 / 242\n",
      "LR: 3.808303274179329e-05\n",
      "Train loss: 4.824676990509033\n",
      "\n",
      "Time (s): 0.48587584495544434\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 219 / 242\n",
      "LR: 3.825772555253547e-05\n",
      "Train loss: 4.821853160858154\n",
      "\n",
      "Time (s): 0.4810318946838379\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 220 / 242\n",
      "LR: 3.8432418363277636e-05\n",
      "Train loss: 4.813895225524902\n",
      "\n",
      "Time (s): 0.48459529876708984\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 221 / 242\n",
      "LR: 3.860711117401981e-05\n",
      "Train loss: 4.71043062210083\n",
      "\n",
      "Time (s): 0.47687840461730957\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 222 / 242\n",
      "LR: 3.878180398476198e-05\n",
      "Train loss: 4.792518615722656\n",
      "\n",
      "Time (s): 0.48357343673706055\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 223 / 242\n",
      "LR: 3.895649679550415e-05\n",
      "Train loss: 4.7236480712890625\n",
      "\n",
      "Time (s): 0.48264408111572266\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 224 / 242\n",
      "LR: 3.913118960624632e-05\n",
      "Train loss: 4.894157409667969\n",
      "\n",
      "Time (s): 0.4830207824707031\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 225 / 242\n",
      "LR: 3.930588241698849e-05\n",
      "Train loss: 4.771275043487549\n",
      "\n",
      "Time (s): 0.47423315048217773\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 226 / 242\n",
      "LR: 3.9480575227730664e-05\n",
      "Train loss: 4.739782333374023\n",
      "\n",
      "Time (s): 0.4696657657623291\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 227 / 242\n",
      "LR: 3.965526803847283e-05\n",
      "Train loss: 4.608099937438965\n",
      "\n",
      "Time (s): 0.4846174716949463\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 228 / 242\n",
      "LR: 3.9829960849215006e-05\n",
      "Train loss: 4.638624668121338\n",
      "\n",
      "Time (s): 0.48291921615600586\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 229 / 242\n",
      "LR: 4.0004653659957174e-05\n",
      "Train loss: 4.676361560821533\n",
      "\n",
      "Time (s): 0.4956626892089844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 230 / 242\n",
      "LR: 4.017934647069935e-05\n",
      "Train loss: 4.594430446624756\n",
      "\n",
      "Time (s): 0.4860832691192627\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 231 / 242\n",
      "LR: 4.035403928144152e-05\n",
      "Train loss: 4.47422456741333\n",
      "\n",
      "Time (s): 0.48709607124328613\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 232 / 242\n",
      "LR: 4.052873209218369e-05\n",
      "Train loss: 4.616684436798096\n",
      "\n",
      "Time (s): 0.48901796340942383\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 233 / 242\n",
      "LR: 4.070342490292586e-05\n",
      "Train loss: 4.547407150268555\n",
      "\n",
      "Time (s): 0.5230832099914551\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 234 / 242\n",
      "LR: 4.0878117713668034e-05\n",
      "Train loss: 4.645047187805176\n",
      "\n",
      "Time (s): 0.509608268737793\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 235 / 242\n",
      "LR: 4.10528105244102e-05\n",
      "Train loss: 4.485249996185303\n",
      "\n",
      "Time (s): 0.47914648056030273\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 236 / 242\n",
      "LR: 4.1227503335152376e-05\n",
      "Train loss: 4.70672607421875\n",
      "\n",
      "Time (s): 0.48697423934936523\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 237 / 242\n",
      "LR: 4.1402196145894544e-05\n",
      "Train loss: 4.594146251678467\n",
      "\n",
      "Time (s): 0.4853637218475342\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 238 / 242\n",
      "LR: 4.157688895663672e-05\n",
      "Train loss: 4.534954071044922\n",
      "\n",
      "Time (s): 0.4970855712890625\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 239 / 242\n",
      "LR: 4.175158176737889e-05\n",
      "Train loss: 4.582169532775879\n",
      "\n",
      "Time (s): 0.508547306060791\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 240 / 242\n",
      "LR: 4.1926274578121055e-05\n",
      "Train loss: 4.511336803436279\n",
      "\n",
      "Time (s): 0.4919109344482422\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 241 / 242\n",
      "LR: 4.210096738886323e-05\n",
      "Train loss: 4.641180992126465\n",
      "\n",
      "Time (s): 0.48954224586486816\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 1  Batch 242 / 242\n",
      "LR: 4.22756601996054e-05\n",
      "Train loss: 4.5128655433654785\n",
      "\n",
      "Time (s): 0.4867129325866699\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Evaluating:\n",
      "Epoch: 1\n",
      "Avg train loss: 4.453183195807717\n",
      "Avg train acc: 0.07432301838166458\n",
      "Avg eval loss: 4.471226395501031\n",
      "Avg eval acc: 0.07499878522422579\n",
      "=========================\n",
      "\n",
      "\n",
      "=========================\n",
      "NEW EPOCH: 2\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 1 / 242\n",
      "LR: 4.245035301034757e-05\n",
      "Train loss: 4.545709609985352\n",
      "\n",
      "Time (s): 0.5280652046203613\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 2 / 242\n",
      "LR: 4.262504582108974e-05\n",
      "Train loss: 4.342453479766846\n",
      "\n",
      "Time (s): 0.501352071762085\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 3 / 242\n",
      "LR: 4.2799738631831915e-05\n",
      "Train loss: 4.589694023132324\n",
      "\n",
      "Time (s): 0.4797821044921875\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 4 / 242\n",
      "LR: 4.297443144257408e-05\n",
      "Train loss: 4.439275741577148\n",
      "\n",
      "Time (s): 0.4849534034729004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 5 / 242\n",
      "LR: 4.314912425331626e-05\n",
      "Train loss: 4.368325233459473\n",
      "\n",
      "Time (s): 0.47999095916748047\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 6 / 242\n",
      "LR: 4.3323817064058425e-05\n",
      "Train loss: 4.44576358795166\n",
      "\n",
      "Time (s): 0.47524094581604004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 7 / 242\n",
      "LR: 4.34985098748006e-05\n",
      "Train loss: 4.442349433898926\n",
      "\n",
      "Time (s): 0.482496976852417\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 8 / 242\n",
      "LR: 4.367320268554277e-05\n",
      "Train loss: 4.455574989318848\n",
      "\n",
      "Time (s): 0.4876670837402344\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 9 / 242\n",
      "LR: 4.384789549628494e-05\n",
      "Train loss: 4.508020401000977\n",
      "\n",
      "Time (s): 0.4856233596801758\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 10 / 242\n",
      "LR: 4.402258830702711e-05\n",
      "Train loss: 4.371882915496826\n",
      "\n",
      "Time (s): 0.49320149421691895\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 11 / 242\n",
      "LR: 4.4197281117769285e-05\n",
      "Train loss: 4.340822696685791\n",
      "\n",
      "Time (s): 0.4815857410430908\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 12 / 242\n",
      "LR: 4.437197392851145e-05\n",
      "Train loss: 4.430503845214844\n",
      "\n",
      "Time (s): 0.5153920650482178\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 13 / 242\n",
      "LR: 4.454666673925363e-05\n",
      "Train loss: 4.417754173278809\n",
      "\n",
      "Time (s): 0.4768400192260742\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 14 / 242\n",
      "LR: 4.4721359549995795e-05\n",
      "Train loss: 4.348806381225586\n",
      "\n",
      "Time (s): 0.4726090431213379\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 15 / 242\n",
      "LR: 4.489605236073796e-05\n",
      "Train loss: 4.5661468505859375\n",
      "\n",
      "Time (s): 0.48186683654785156\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 16 / 242\n",
      "LR: 4.507074517148014e-05\n",
      "Train loss: 4.382812023162842\n",
      "\n",
      "Time (s): 0.4761769771575928\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 17 / 242\n",
      "LR: 4.5245437982222306e-05\n",
      "Train loss: 4.567455768585205\n",
      "\n",
      "Time (s): 0.48602795600891113\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 18 / 242\n",
      "LR: 4.542013079296448e-05\n",
      "Train loss: 4.682069301605225\n",
      "\n",
      "Time (s): 0.4851236343383789\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 19 / 242\n",
      "LR: 4.559482360370665e-05\n",
      "Train loss: 4.542210578918457\n",
      "\n",
      "Time (s): 0.482466459274292\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 20 / 242\n",
      "LR: 4.576951641444882e-05\n",
      "Train loss: 4.57888650894165\n",
      "\n",
      "Time (s): 0.4823579788208008\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 21 / 242\n",
      "LR: 4.594420922519099e-05\n",
      "Train loss: 4.412408351898193\n",
      "\n",
      "Time (s): 0.4765899181365967\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 22 / 242\n",
      "LR: 4.6118902035933166e-05\n",
      "Train loss: 4.488194465637207\n",
      "\n",
      "Time (s): 0.4866642951965332\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 23 / 242\n",
      "LR: 4.6293594846675334e-05\n",
      "Train loss: 4.451239109039307\n",
      "\n",
      "Time (s): 0.5100114345550537\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 24 / 242\n",
      "LR: 4.646828765741751e-05\n",
      "Train loss: 4.420692443847656\n",
      "\n",
      "Time (s): 0.4909236431121826\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 25 / 242\n",
      "LR: 4.6642980468159676e-05\n",
      "Train loss: 4.304290294647217\n",
      "\n",
      "Time (s): 0.48438119888305664\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 26 / 242\n",
      "LR: 4.681767327890185e-05\n",
      "Train loss: 4.33042049407959\n",
      "\n",
      "Time (s): 0.47930073738098145\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 27 / 242\n",
      "LR: 4.699236608964402e-05\n",
      "Train loss: 4.247732162475586\n",
      "\n",
      "Time (s): 0.48401689529418945\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 28 / 242\n",
      "LR: 4.7167058900386193e-05\n",
      "Train loss: 4.3383989334106445\n",
      "\n",
      "Time (s): 0.4849534034729004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 29 / 242\n",
      "LR: 4.734175171112836e-05\n",
      "Train loss: 4.312528133392334\n",
      "\n",
      "Time (s): 0.48219799995422363\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 30 / 242\n",
      "LR: 4.7516444521870536e-05\n",
      "Train loss: 4.366162300109863\n",
      "\n",
      "Time (s): 0.48258447647094727\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 31 / 242\n",
      "LR: 4.7691137332612704e-05\n",
      "Train loss: 4.340944766998291\n",
      "\n",
      "Time (s): 0.4857819080352783\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 32 / 242\n",
      "LR: 4.786583014335487e-05\n",
      "Train loss: 4.333026885986328\n",
      "\n",
      "Time (s): 0.48417210578918457\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 33 / 242\n",
      "LR: 4.8040522954097047e-05\n",
      "Train loss: 4.2340240478515625\n",
      "\n",
      "Time (s): 0.49634623527526855\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 34 / 242\n",
      "LR: 4.8215215764839214e-05\n",
      "Train loss: 4.53609561920166\n",
      "\n",
      "Time (s): 0.4813351631164551\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 35 / 242\n",
      "LR: 4.838990857558139e-05\n",
      "Train loss: 4.3973822593688965\n",
      "\n",
      "Time (s): 0.4828002452850342\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 36 / 242\n",
      "LR: 4.856460138632356e-05\n",
      "Train loss: 4.237080097198486\n",
      "\n",
      "Time (s): 0.4833238124847412\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 37 / 242\n",
      "LR: 4.873929419706573e-05\n",
      "Train loss: 4.422794342041016\n",
      "\n",
      "Time (s): 0.4842071533203125\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 38 / 242\n",
      "LR: 4.89139870078079e-05\n",
      "Train loss: 4.195932388305664\n",
      "\n",
      "Time (s): 0.49044108390808105\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 39 / 242\n",
      "LR: 4.9088679818550074e-05\n",
      "Train loss: 4.254065036773682\n",
      "\n",
      "Time (s): 0.4821817874908447\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 40 / 242\n",
      "LR: 4.926337262929224e-05\n",
      "Train loss: 4.383701324462891\n",
      "\n",
      "Time (s): 0.4850730895996094\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 41 / 242\n",
      "LR: 4.943806544003442e-05\n",
      "Train loss: 4.269065856933594\n",
      "\n",
      "Time (s): 0.484025239944458\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 42 / 242\n",
      "LR: 4.9612758250776585e-05\n",
      "Train loss: 4.394236087799072\n",
      "\n",
      "Time (s): 0.4871189594268799\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 43 / 242\n",
      "LR: 4.978745106151876e-05\n",
      "Train loss: 4.145735263824463\n",
      "\n",
      "Time (s): 0.48884129524230957\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 44 / 242\n",
      "LR: 4.996214387226093e-05\n",
      "Train loss: 4.342783451080322\n",
      "\n",
      "Time (s): 0.4948902130126953\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 45 / 242\n",
      "LR: 5.01368366830031e-05\n",
      "Train loss: 4.320735931396484\n",
      "\n",
      "Time (s): 0.48761606216430664\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 46 / 242\n",
      "LR: 5.031152949374527e-05\n",
      "Train loss: 4.252046585083008\n",
      "\n",
      "Time (s): 0.49228501319885254\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 47 / 242\n",
      "LR: 5.048622230448744e-05\n",
      "Train loss: 4.281364440917969\n",
      "\n",
      "Time (s): 0.48650574684143066\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 48 / 242\n",
      "LR: 5.066091511522961e-05\n",
      "Train loss: 4.119751930236816\n",
      "\n",
      "Time (s): 0.47812390327453613\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 49 / 242\n",
      "LR: 5.083560792597178e-05\n",
      "Train loss: 4.361525058746338\n",
      "\n",
      "Time (s): 0.48807644844055176\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 50 / 242\n",
      "LR: 5.1010300736713955e-05\n",
      "Train loss: 4.20952844619751\n",
      "\n",
      "Time (s): 0.48529934883117676\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 51 / 242\n",
      "LR: 5.118499354745612e-05\n",
      "Train loss: 4.271289348602295\n",
      "\n",
      "Time (s): 0.48497605323791504\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 52 / 242\n",
      "LR: 5.13596863581983e-05\n",
      "Train loss: 4.241422653198242\n",
      "\n",
      "Time (s): 0.4827914237976074\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 53 / 242\n",
      "LR: 5.1534379168940466e-05\n",
      "Train loss: 4.3046393394470215\n",
      "\n",
      "Time (s): 0.48879575729370117\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 54 / 242\n",
      "LR: 5.170907197968264e-05\n",
      "Train loss: 4.180194854736328\n",
      "\n",
      "Time (s): 0.48360490798950195\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 55 / 242\n",
      "LR: 5.188376479042481e-05\n",
      "Train loss: 4.355859756469727\n",
      "\n",
      "Time (s): 0.4858133792877197\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 56 / 242\n",
      "LR: 5.205845760116698e-05\n",
      "Train loss: 4.256612300872803\n",
      "\n",
      "Time (s): 0.4861173629760742\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 57 / 242\n",
      "LR: 5.223315041190915e-05\n",
      "Train loss: 4.2744269371032715\n",
      "\n",
      "Time (s): 0.4862532615661621\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 58 / 242\n",
      "LR: 5.2407843222651325e-05\n",
      "Train loss: 4.281702041625977\n",
      "\n",
      "Time (s): 0.47458386421203613\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 59 / 242\n",
      "LR: 5.258253603339349e-05\n",
      "Train loss: 4.287638187408447\n",
      "\n",
      "Time (s): 0.48522472381591797\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 60 / 242\n",
      "LR: 5.275722884413567e-05\n",
      "Train loss: 4.331732749938965\n",
      "\n",
      "Time (s): 0.4869120121002197\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 61 / 242\n",
      "LR: 5.2931921654877836e-05\n",
      "Train loss: 4.1825480461120605\n",
      "\n",
      "Time (s): 0.48634767532348633\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 62 / 242\n",
      "LR: 5.3106614465620004e-05\n",
      "Train loss: 4.237563610076904\n",
      "\n",
      "Time (s): 0.48923277854919434\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 63 / 242\n",
      "LR: 5.328130727636218e-05\n",
      "Train loss: 4.181135654449463\n",
      "\n",
      "Time (s): 0.4894998073577881\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 64 / 242\n",
      "LR: 5.3456000087104346e-05\n",
      "Train loss: 4.338724136352539\n",
      "\n",
      "Time (s): 0.4883005619049072\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 65 / 242\n",
      "LR: 5.363069289784652e-05\n",
      "Train loss: 4.24538516998291\n",
      "\n",
      "Time (s): 0.4854724407196045\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 66 / 242\n",
      "LR: 5.380538570858869e-05\n",
      "Train loss: 4.22284460067749\n",
      "\n",
      "Time (s): 0.47710561752319336\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 67 / 242\n",
      "LR: 5.3980078519330864e-05\n",
      "Train loss: 4.24772310256958\n",
      "\n",
      "Time (s): 0.47771477699279785\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 68 / 242\n",
      "LR: 5.415477133007303e-05\n",
      "Train loss: 4.081892490386963\n",
      "\n",
      "Time (s): 0.4843919277191162\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 69 / 242\n",
      "LR: 5.4329464140815206e-05\n",
      "Train loss: 4.209270477294922\n",
      "\n",
      "Time (s): 0.48259782791137695\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 70 / 242\n",
      "LR: 5.4504156951557374e-05\n",
      "Train loss: 4.1835761070251465\n",
      "\n",
      "Time (s): 0.47378110885620117\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 71 / 242\n",
      "LR: 5.467884976229955e-05\n",
      "Train loss: 4.2884650230407715\n",
      "\n",
      "Time (s): 0.4785878658294678\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 72 / 242\n",
      "LR: 5.4853542573041717e-05\n",
      "Train loss: 4.32937479019165\n",
      "\n",
      "Time (s): 0.472381591796875\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 73 / 242\n",
      "LR: 5.502823538378389e-05\n",
      "Train loss: 4.368083477020264\n",
      "\n",
      "Time (s): 0.4725477695465088\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 74 / 242\n",
      "LR: 5.520292819452606e-05\n",
      "Train loss: 4.154061794281006\n",
      "\n",
      "Time (s): 0.4855844974517822\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 75 / 242\n",
      "LR: 5.5377621005268234e-05\n",
      "Train loss: 4.206934452056885\n",
      "\n",
      "Time (s): 0.49097609519958496\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 76 / 242\n",
      "LR: 5.55523138160104e-05\n",
      "Train loss: 4.242754936218262\n",
      "\n",
      "Time (s): 0.4812800884246826\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 77 / 242\n",
      "LR: 5.5727006626752576e-05\n",
      "Train loss: 4.301383018493652\n",
      "\n",
      "Time (s): 0.48720550537109375\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 78 / 242\n",
      "LR: 5.5901699437494744e-05\n",
      "Train loss: 4.269822597503662\n",
      "\n",
      "Time (s): 0.5089285373687744\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 79 / 242\n",
      "LR: 5.607639224823691e-05\n",
      "Train loss: 4.273266315460205\n",
      "\n",
      "Time (s): 0.4847438335418701\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 80 / 242\n",
      "LR: 5.625108505897909e-05\n",
      "Train loss: 4.288502216339111\n",
      "\n",
      "Time (s): 0.5010330677032471\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 81 / 242\n",
      "LR: 5.6425777869721255e-05\n",
      "Train loss: 4.079207420349121\n",
      "\n",
      "Time (s): 0.4971048831939697\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 82 / 242\n",
      "LR: 5.660047068046343e-05\n",
      "Train loss: 4.16061544418335\n",
      "\n",
      "Time (s): 0.490189790725708\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 83 / 242\n",
      "LR: 5.67751634912056e-05\n",
      "Train loss: 4.282474517822266\n",
      "\n",
      "Time (s): 0.48664093017578125\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 84 / 242\n",
      "LR: 5.694985630194777e-05\n",
      "Train loss: 4.126003265380859\n",
      "\n",
      "Time (s): 0.48574185371398926\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 85 / 242\n",
      "LR: 5.712454911268994e-05\n",
      "Train loss: 4.1427001953125\n",
      "\n",
      "Time (s): 0.511587381362915\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 86 / 242\n",
      "LR: 5.7299241923432115e-05\n",
      "Train loss: 4.173567771911621\n",
      "\n",
      "Time (s): 0.4835543632507324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 87 / 242\n",
      "LR: 5.747393473417428e-05\n",
      "Train loss: 4.176908493041992\n",
      "\n",
      "Time (s): 0.47722887992858887\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 88 / 242\n",
      "LR: 5.764862754491646e-05\n",
      "Train loss: 4.232720375061035\n",
      "\n",
      "Time (s): 0.4879131317138672\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 89 / 242\n",
      "LR: 5.7823320355658625e-05\n",
      "Train loss: 4.11494255065918\n",
      "\n",
      "Time (s): 0.4851856231689453\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 90 / 242\n",
      "LR: 5.79980131664008e-05\n",
      "Train loss: 4.090475559234619\n",
      "\n",
      "Time (s): 0.499591588973999\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 91 / 242\n",
      "LR: 5.817270597714297e-05\n",
      "Train loss: 4.34018087387085\n",
      "\n",
      "Time (s): 0.4834787845611572\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 92 / 242\n",
      "LR: 5.834739878788514e-05\n",
      "Train loss: 4.215574741363525\n",
      "\n",
      "Time (s): 0.4893534183502197\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 93 / 242\n",
      "LR: 5.852209159862731e-05\n",
      "Train loss: 4.165215015411377\n",
      "\n",
      "Time (s): 0.48629093170166016\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 94 / 242\n",
      "LR: 5.8696784409369485e-05\n",
      "Train loss: 4.1153178215026855\n",
      "\n",
      "Time (s): 0.48753833770751953\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 95 / 242\n",
      "LR: 5.887147722011165e-05\n",
      "Train loss: 4.093364238739014\n",
      "\n",
      "Time (s): 0.47968149185180664\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 96 / 242\n",
      "LR: 5.904617003085382e-05\n",
      "Train loss: 4.174981117248535\n",
      "\n",
      "Time (s): 0.4776735305786133\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 97 / 242\n",
      "LR: 5.9220862841595995e-05\n",
      "Train loss: 4.189924716949463\n",
      "\n",
      "Time (s): 0.4685807228088379\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 98 / 242\n",
      "LR: 5.939555565233816e-05\n",
      "Train loss: 4.172794342041016\n",
      "\n",
      "Time (s): 0.4756603240966797\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 99 / 242\n",
      "LR: 5.957024846308034e-05\n",
      "Train loss: 4.274876117706299\n",
      "\n",
      "Time (s): 0.4692671298980713\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 100 / 242\n",
      "LR: 5.9744941273822506e-05\n",
      "Train loss: 4.390874862670898\n",
      "\n",
      "Time (s): 0.479245662689209\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 101 / 242\n",
      "LR: 5.991963408456468e-05\n",
      "Train loss: 4.07829475402832\n",
      "\n",
      "Time (s): 0.47753024101257324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 102 / 242\n",
      "LR: 6.009432689530685e-05\n",
      "Train loss: 3.9935402870178223\n",
      "\n",
      "Time (s): 0.4745914936065674\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 103 / 242\n",
      "LR: 6.026901970604902e-05\n",
      "Train loss: 4.0232672691345215\n",
      "\n",
      "Time (s): 0.4772322177886963\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 104 / 242\n",
      "LR: 6.044371251679119e-05\n",
      "Train loss: 4.224996566772461\n",
      "\n",
      "Time (s): 0.47470831871032715\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 105 / 242\n",
      "LR: 6.0618405327533366e-05\n",
      "Train loss: 4.002597808837891\n",
      "\n",
      "Time (s): 0.47727179527282715\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 106 / 242\n",
      "LR: 6.0793098138275534e-05\n",
      "Train loss: 4.20125150680542\n",
      "\n",
      "Time (s): 0.4788045883178711\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 107 / 242\n",
      "LR: 6.096779094901771e-05\n",
      "Train loss: 4.238633155822754\n",
      "\n",
      "Time (s): 0.47423434257507324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 108 / 242\n",
      "LR: 6.114248375975988e-05\n",
      "Train loss: 4.11474084854126\n",
      "\n",
      "Time (s): 0.4695558547973633\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 109 / 242\n",
      "LR: 6.131717657050204e-05\n",
      "Train loss: 4.0760674476623535\n",
      "\n",
      "Time (s): 0.4780845642089844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 110 / 242\n",
      "LR: 6.149186938124421e-05\n",
      "Train loss: 4.214532375335693\n",
      "\n",
      "Time (s): 0.47723937034606934\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 111 / 242\n",
      "LR: 6.16665621919864e-05\n",
      "Train loss: 4.044397830963135\n",
      "\n",
      "Time (s): 0.4782066345214844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 112 / 242\n",
      "LR: 6.184125500272856e-05\n",
      "Train loss: 4.12691068649292\n",
      "\n",
      "Time (s): 0.4782748222351074\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 113 / 242\n",
      "LR: 6.201594781347073e-05\n",
      "Train loss: 4.092891693115234\n",
      "\n",
      "Time (s): 0.4805896282196045\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 114 / 242\n",
      "LR: 6.21906406242129e-05\n",
      "Train loss: 4.163064956665039\n",
      "\n",
      "Time (s): 0.46726346015930176\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 115 / 242\n",
      "LR: 6.236533343495508e-05\n",
      "Train loss: 4.33109188079834\n",
      "\n",
      "Time (s): 0.47623729705810547\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 116 / 242\n",
      "LR: 6.254002624569725e-05\n",
      "Train loss: 4.1562066078186035\n",
      "\n",
      "Time (s): 0.4708061218261719\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 117 / 242\n",
      "LR: 6.271471905643941e-05\n",
      "Train loss: 4.192769527435303\n",
      "\n",
      "Time (s): 0.47768139839172363\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 118 / 242\n",
      "LR: 6.288941186718158e-05\n",
      "Train loss: 4.3794426918029785\n",
      "\n",
      "Time (s): 0.4826667308807373\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 119 / 242\n",
      "LR: 6.306410467792376e-05\n",
      "Train loss: 4.076939105987549\n",
      "\n",
      "Time (s): 0.47734570503234863\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 120 / 242\n",
      "LR: 6.323879748866593e-05\n",
      "Train loss: 4.182668209075928\n",
      "\n",
      "Time (s): 0.47892165184020996\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 121 / 242\n",
      "LR: 6.34134902994081e-05\n",
      "Train loss: 4.063438415527344\n",
      "\n",
      "Time (s): 0.4807868003845215\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 122 / 242\n",
      "LR: 6.358818311015027e-05\n",
      "Train loss: 4.059777736663818\n",
      "\n",
      "Time (s): 0.47121739387512207\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 123 / 242\n",
      "LR: 6.376287592089245e-05\n",
      "Train loss: 4.1774516105651855\n",
      "\n",
      "Time (s): 0.4794142246246338\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 124 / 242\n",
      "LR: 6.393756873163462e-05\n",
      "Train loss: 4.135895729064941\n",
      "\n",
      "Time (s): 0.48055100440979004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 125 / 242\n",
      "LR: 6.411226154237678e-05\n",
      "Train loss: 4.165460586547852\n",
      "\n",
      "Time (s): 0.47121667861938477\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 126 / 242\n",
      "LR: 6.428695435311895e-05\n",
      "Train loss: 4.139952659606934\n",
      "\n",
      "Time (s): 0.4789419174194336\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 127 / 242\n",
      "LR: 6.446164716386112e-05\n",
      "Train loss: 4.086128234863281\n",
      "\n",
      "Time (s): 0.48149561882019043\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 128 / 242\n",
      "LR: 6.46363399746033e-05\n",
      "Train loss: 4.243580341339111\n",
      "\n",
      "Time (s): 0.4785499572753906\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 129 / 242\n",
      "LR: 6.481103278534547e-05\n",
      "Train loss: 4.186905384063721\n",
      "\n",
      "Time (s): 0.47879719734191895\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 130 / 242\n",
      "LR: 6.498572559608764e-05\n",
      "Train loss: 4.116554260253906\n",
      "\n",
      "Time (s): 0.4784965515136719\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 131 / 242\n",
      "LR: 6.51604184068298e-05\n",
      "Train loss: 4.01713752746582\n",
      "\n",
      "Time (s): 0.4786980152130127\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 132 / 242\n",
      "LR: 6.533511121757199e-05\n",
      "Train loss: 4.121999740600586\n",
      "\n",
      "Time (s): 0.49416589736938477\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 133 / 242\n",
      "LR: 6.550980402831415e-05\n",
      "Train loss: 4.201040267944336\n",
      "\n",
      "Time (s): 0.48414158821105957\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 134 / 242\n",
      "LR: 6.568449683905632e-05\n",
      "Train loss: 4.230140686035156\n",
      "\n",
      "Time (s): 0.48083043098449707\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 135 / 242\n",
      "LR: 6.585918964979849e-05\n",
      "Train loss: 4.030828475952148\n",
      "\n",
      "Time (s): 0.4795079231262207\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 136 / 242\n",
      "LR: 6.603388246054067e-05\n",
      "Train loss: 4.027429580688477\n",
      "\n",
      "Time (s): 0.47864460945129395\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 137 / 242\n",
      "LR: 6.620857527128284e-05\n",
      "Train loss: 4.068596839904785\n",
      "\n",
      "Time (s): 0.47985315322875977\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 138 / 242\n",
      "LR: 6.638326808202501e-05\n",
      "Train loss: 4.10813570022583\n",
      "\n",
      "Time (s): 0.4790189266204834\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 139 / 242\n",
      "LR: 6.655796089276718e-05\n",
      "Train loss: 4.112785339355469\n",
      "\n",
      "Time (s): 0.4761166572570801\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 140 / 242\n",
      "LR: 6.673265370350936e-05\n",
      "Train loss: 4.169009208679199\n",
      "\n",
      "Time (s): 0.48364782333374023\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 141 / 242\n",
      "LR: 6.690734651425153e-05\n",
      "Train loss: 4.120504379272461\n",
      "\n",
      "Time (s): 0.4692981243133545\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 142 / 242\n",
      "LR: 6.708203932499369e-05\n",
      "Train loss: 4.197935581207275\n",
      "\n",
      "Time (s): 0.4799623489379883\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 143 / 242\n",
      "LR: 6.725673213573586e-05\n",
      "Train loss: 4.164944648742676\n",
      "\n",
      "Time (s): 0.47931504249572754\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 144 / 242\n",
      "LR: 6.743142494647803e-05\n",
      "Train loss: 4.078322887420654\n",
      "\n",
      "Time (s): 0.47533726692199707\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 145 / 242\n",
      "LR: 6.760611775722021e-05\n",
      "Train loss: 3.952714681625366\n",
      "\n",
      "Time (s): 0.472259521484375\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 146 / 242\n",
      "LR: 6.778081056796238e-05\n",
      "Train loss: 4.208882808685303\n",
      "\n",
      "Time (s): 0.4763607978820801\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 147 / 242\n",
      "LR: 6.795550337870455e-05\n",
      "Train loss: 4.20068883895874\n",
      "\n",
      "Time (s): 0.4693796634674072\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 148 / 242\n",
      "LR: 6.813019618944671e-05\n",
      "Train loss: 3.9896934032440186\n",
      "\n",
      "Time (s): 0.47303175926208496\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 149 / 242\n",
      "LR: 6.83048890001889e-05\n",
      "Train loss: 4.096466541290283\n",
      "\n",
      "Time (s): 0.4822373390197754\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 150 / 242\n",
      "LR: 6.847958181093106e-05\n",
      "Train loss: 4.205097198486328\n",
      "\n",
      "Time (s): 0.47840356826782227\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 151 / 242\n",
      "LR: 6.865427462167323e-05\n",
      "Train loss: 4.22681999206543\n",
      "\n",
      "Time (s): 0.4777953624725342\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 152 / 242\n",
      "LR: 6.88289674324154e-05\n",
      "Train loss: 4.0363664627075195\n",
      "\n",
      "Time (s): 0.4693272113800049\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 153 / 242\n",
      "LR: 6.900366024315758e-05\n",
      "Train loss: 3.974903106689453\n",
      "\n",
      "Time (s): 0.47765183448791504\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 154 / 242\n",
      "LR: 6.917835305389975e-05\n",
      "Train loss: 4.096748352050781\n",
      "\n",
      "Time (s): 0.4691627025604248\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 155 / 242\n",
      "LR: 6.935304586464192e-05\n",
      "Train loss: 4.077974319458008\n",
      "\n",
      "Time (s): 0.47600436210632324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 156 / 242\n",
      "LR: 6.952773867538408e-05\n",
      "Train loss: 3.913773536682129\n",
      "\n",
      "Time (s): 0.4735112190246582\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 157 / 242\n",
      "LR: 6.970243148612627e-05\n",
      "Train loss: 4.1511993408203125\n",
      "\n",
      "Time (s): 0.47631382942199707\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 158 / 242\n",
      "LR: 6.987712429686843e-05\n",
      "Train loss: 4.054157257080078\n",
      "\n",
      "Time (s): 0.4756002426147461\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 159 / 242\n",
      "LR: 7.00518171076106e-05\n",
      "Train loss: 4.094803810119629\n",
      "\n",
      "Time (s): 0.476715087890625\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 160 / 242\n",
      "LR: 7.022650991835277e-05\n",
      "Train loss: 4.2529730796813965\n",
      "\n",
      "Time (s): 0.4767639636993408\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 161 / 242\n",
      "LR: 7.040120272909494e-05\n",
      "Train loss: 4.116408348083496\n",
      "\n",
      "Time (s): 0.5010695457458496\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 162 / 242\n",
      "LR: 7.057589553983712e-05\n",
      "Train loss: 4.200219631195068\n",
      "\n",
      "Time (s): 0.48062920570373535\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 163 / 242\n",
      "LR: 7.075058835057929e-05\n",
      "Train loss: 3.915955066680908\n",
      "\n",
      "Time (s): 0.4819784164428711\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 164 / 242\n",
      "LR: 7.092528116132145e-05\n",
      "Train loss: 4.324817657470703\n",
      "\n",
      "Time (s): 0.47925639152526855\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 165 / 242\n",
      "LR: 7.109997397206362e-05\n",
      "Train loss: 4.10778284072876\n",
      "\n",
      "Time (s): 0.4795682430267334\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 166 / 242\n",
      "LR: 7.12746667828058e-05\n",
      "Train loss: 4.044834136962891\n",
      "\n",
      "Time (s): 0.4779469966888428\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 167 / 242\n",
      "LR: 7.144935959354797e-05\n",
      "Train loss: 4.229884624481201\n",
      "\n",
      "Time (s): 0.4822852611541748\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 168 / 242\n",
      "LR: 7.162405240429014e-05\n",
      "Train loss: 3.996156930923462\n",
      "\n",
      "Time (s): 0.4809994697570801\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 169 / 242\n",
      "LR: 7.179874521503231e-05\n",
      "Train loss: 4.2967352867126465\n",
      "\n",
      "Time (s): 0.4784884452819824\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 170 / 242\n",
      "LR: 7.197343802577449e-05\n",
      "Train loss: 4.330080509185791\n",
      "\n",
      "Time (s): 0.5163452625274658\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 171 / 242\n",
      "LR: 7.214813083651666e-05\n",
      "Train loss: 4.151904106140137\n",
      "\n",
      "Time (s): 0.49887728691101074\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 172 / 242\n",
      "LR: 7.232282364725883e-05\n",
      "Train loss: 3.9012022018432617\n",
      "\n",
      "Time (s): 0.47437214851379395\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 173 / 242\n",
      "LR: 7.249751645800099e-05\n",
      "Train loss: 4.055342674255371\n",
      "\n",
      "Time (s): 0.4772529602050781\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 174 / 242\n",
      "LR: 7.267220926874317e-05\n",
      "Train loss: 4.1283955574035645\n",
      "\n",
      "Time (s): 0.47499775886535645\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 175 / 242\n",
      "LR: 7.284690207948534e-05\n",
      "Train loss: 4.0072736740112305\n",
      "\n",
      "Time (s): 0.47757482528686523\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 176 / 242\n",
      "LR: 7.302159489022751e-05\n",
      "Train loss: 4.120522975921631\n",
      "\n",
      "Time (s): 0.47942042350769043\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 177 / 242\n",
      "LR: 7.319628770096968e-05\n",
      "Train loss: 4.010710716247559\n",
      "\n",
      "Time (s): 0.4789695739746094\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 178 / 242\n",
      "LR: 7.337098051171185e-05\n",
      "Train loss: 3.958944082260132\n",
      "\n",
      "Time (s): 0.4762003421783447\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 179 / 242\n",
      "LR: 7.354567332245403e-05\n",
      "Train loss: 4.208222389221191\n",
      "\n",
      "Time (s): 0.4730370044708252\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 180 / 242\n",
      "LR: 7.37203661331962e-05\n",
      "Train loss: 4.0178046226501465\n",
      "\n",
      "Time (s): 0.4904048442840576\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 181 / 242\n",
      "LR: 7.389505894393836e-05\n",
      "Train loss: 4.055988788604736\n",
      "\n",
      "Time (s): 0.4887833595275879\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 182 / 242\n",
      "LR: 7.406975175468053e-05\n",
      "Train loss: 4.2426252365112305\n",
      "\n",
      "Time (s): 0.47803568840026855\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 183 / 242\n",
      "LR: 7.424444456542271e-05\n",
      "Train loss: 4.211060047149658\n",
      "\n",
      "Time (s): 0.48912549018859863\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 184 / 242\n",
      "LR: 7.441913737616488e-05\n",
      "Train loss: 4.231321811676025\n",
      "\n",
      "Time (s): 0.48004603385925293\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 185 / 242\n",
      "LR: 7.459383018690705e-05\n",
      "Train loss: 3.9652843475341797\n",
      "\n",
      "Time (s): 0.4819326400756836\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 186 / 242\n",
      "LR: 7.476852299764922e-05\n",
      "Train loss: 4.003176212310791\n",
      "\n",
      "Time (s): 0.4723243713378906\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 187 / 242\n",
      "LR: 7.49432158083914e-05\n",
      "Train loss: 4.160190582275391\n",
      "\n",
      "Time (s): 0.5031778812408447\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 188 / 242\n",
      "LR: 7.511790861913357e-05\n",
      "Train loss: 4.190059185028076\n",
      "\n",
      "Time (s): 0.47064852714538574\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 189 / 242\n",
      "LR: 7.529260142987573e-05\n",
      "Train loss: 4.024954319000244\n",
      "\n",
      "Time (s): 0.47722363471984863\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 190 / 242\n",
      "LR: 7.54672942406179e-05\n",
      "Train loss: 4.0587897300720215\n",
      "\n",
      "Time (s): 0.4785788059234619\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 191 / 242\n",
      "LR: 7.564198705136007e-05\n",
      "Train loss: 4.035046100616455\n",
      "\n",
      "Time (s): 0.503565788269043\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 192 / 242\n",
      "LR: 7.581667986210225e-05\n",
      "Train loss: 4.044613361358643\n",
      "\n",
      "Time (s): 0.47602272033691406\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 193 / 242\n",
      "LR: 7.599137267284442e-05\n",
      "Train loss: 4.131071090698242\n",
      "\n",
      "Time (s): 0.5019550323486328\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 194 / 242\n",
      "LR: 7.616606548358659e-05\n",
      "Train loss: 4.3038811683654785\n",
      "\n",
      "Time (s): 0.47667455673217773\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 195 / 242\n",
      "LR: 7.634075829432875e-05\n",
      "Train loss: 4.085091590881348\n",
      "\n",
      "Time (s): 0.4730339050292969\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 196 / 242\n",
      "LR: 7.651545110507094e-05\n",
      "Train loss: 4.0780816078186035\n",
      "\n",
      "Time (s): 0.4743802547454834\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 197 / 242\n",
      "LR: 7.66901439158131e-05\n",
      "Train loss: 4.007974624633789\n",
      "\n",
      "Time (s): 0.4706394672393799\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 198 / 242\n",
      "LR: 7.686483672655527e-05\n",
      "Train loss: 4.164175033569336\n",
      "\n",
      "Time (s): 0.4920964241027832\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 199 / 242\n",
      "LR: 7.703952953729744e-05\n",
      "Train loss: 4.134614944458008\n",
      "\n",
      "Time (s): 0.4764280319213867\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 200 / 242\n",
      "LR: 7.721422234803962e-05\n",
      "Train loss: 4.105303764343262\n",
      "\n",
      "Time (s): 0.47223472595214844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 201 / 242\n",
      "LR: 7.738891515878179e-05\n",
      "Train loss: 4.219649314880371\n",
      "\n",
      "Time (s): 0.4799513816833496\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 202 / 242\n",
      "LR: 7.756360796952396e-05\n",
      "Train loss: 4.187605857849121\n",
      "\n",
      "Time (s): 0.4903900623321533\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 203 / 242\n",
      "LR: 7.773830078026612e-05\n",
      "Train loss: 4.136415004730225\n",
      "\n",
      "Time (s): 0.4822957515716553\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 204 / 242\n",
      "LR: 7.79129935910083e-05\n",
      "Train loss: 4.1356635093688965\n",
      "\n",
      "Time (s): 0.4815514087677002\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 205 / 242\n",
      "LR: 7.808768640175047e-05\n",
      "Train loss: 4.122710704803467\n",
      "\n",
      "Time (s): 0.4739975929260254\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 206 / 242\n",
      "LR: 7.826237921249264e-05\n",
      "Train loss: 4.185878276824951\n",
      "\n",
      "Time (s): 0.47844624519348145\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 207 / 242\n",
      "LR: 7.843707202323481e-05\n",
      "Train loss: 4.1134033203125\n",
      "\n",
      "Time (s): 0.47827839851379395\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 208 / 242\n",
      "LR: 7.861176483397698e-05\n",
      "Train loss: 4.236795425415039\n",
      "\n",
      "Time (s): 0.4784986972808838\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 209 / 242\n",
      "LR: 7.878645764471916e-05\n",
      "Train loss: 4.061266899108887\n",
      "\n",
      "Time (s): 0.49189209938049316\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 210 / 242\n",
      "LR: 7.896115045546133e-05\n",
      "Train loss: 4.118652820587158\n",
      "\n",
      "Time (s): 0.5004634857177734\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 211 / 242\n",
      "LR: 7.91358432662035e-05\n",
      "Train loss: 4.098886489868164\n",
      "\n",
      "Time (s): 0.4729039669036865\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 212 / 242\n",
      "LR: 7.931053607694566e-05\n",
      "Train loss: 3.965233564376831\n",
      "\n",
      "Time (s): 0.5039288997650146\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 213 / 242\n",
      "LR: 7.948522888768784e-05\n",
      "Train loss: 3.973433256149292\n",
      "\n",
      "Time (s): 0.47566699981689453\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 214 / 242\n",
      "LR: 7.965992169843001e-05\n",
      "Train loss: 4.0926899909973145\n",
      "\n",
      "Time (s): 0.480701208114624\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 215 / 242\n",
      "LR: 7.983461450917218e-05\n",
      "Train loss: 4.151620388031006\n",
      "\n",
      "Time (s): 0.47991061210632324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 216 / 242\n",
      "LR: 8.000930731991435e-05\n",
      "Train loss: 4.000178337097168\n",
      "\n",
      "Time (s): 0.48871636390686035\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 217 / 242\n",
      "LR: 8.018400013065653e-05\n",
      "Train loss: 4.111640930175781\n",
      "\n",
      "Time (s): 0.49973273277282715\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 218 / 242\n",
      "LR: 8.03586929413987e-05\n",
      "Train loss: 4.233046054840088\n",
      "\n",
      "Time (s): 0.5024094581604004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 219 / 242\n",
      "LR: 8.053338575214087e-05\n",
      "Train loss: 3.9824211597442627\n",
      "\n",
      "Time (s): 0.5021789073944092\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 220 / 242\n",
      "LR: 8.070807856288303e-05\n",
      "Train loss: 3.9710018634796143\n",
      "\n",
      "Time (s): 0.5035884380340576\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 221 / 242\n",
      "LR: 8.088277137362521e-05\n",
      "Train loss: 4.082793235778809\n",
      "\n",
      "Time (s): 0.4839038848876953\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 222 / 242\n",
      "LR: 8.105746418436738e-05\n",
      "Train loss: 3.9734363555908203\n",
      "\n",
      "Time (s): 0.48261094093322754\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 223 / 242\n",
      "LR: 8.123215699510955e-05\n",
      "Train loss: 4.143672943115234\n",
      "\n",
      "Time (s): 0.4778013229370117\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 224 / 242\n",
      "LR: 8.140684980585172e-05\n",
      "Train loss: 3.978740692138672\n",
      "\n",
      "Time (s): 0.47826266288757324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 225 / 242\n",
      "LR: 8.158154261659389e-05\n",
      "Train loss: 3.9317169189453125\n",
      "\n",
      "Time (s): 0.48053646087646484\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 226 / 242\n",
      "LR: 8.175623542733607e-05\n",
      "Train loss: 3.9996979236602783\n",
      "\n",
      "Time (s): 0.48548364639282227\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 227 / 242\n",
      "LR: 8.193092823807824e-05\n",
      "Train loss: 3.989534616470337\n",
      "\n",
      "Time (s): 0.48713016510009766\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 228 / 242\n",
      "LR: 8.21056210488204e-05\n",
      "Train loss: 4.051239013671875\n",
      "\n",
      "Time (s): 0.4893817901611328\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 229 / 242\n",
      "LR: 8.228031385956257e-05\n",
      "Train loss: 3.9533400535583496\n",
      "\n",
      "Time (s): 0.48522424697875977\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 230 / 242\n",
      "LR: 8.245500667030475e-05\n",
      "Train loss: 4.046234130859375\n",
      "\n",
      "Time (s): 0.4783015251159668\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 231 / 242\n",
      "LR: 8.262969948104692e-05\n",
      "Train loss: 4.21090030670166\n",
      "\n",
      "Time (s): 0.4773578643798828\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 232 / 242\n",
      "LR: 8.280439229178909e-05\n",
      "Train loss: 4.187814712524414\n",
      "\n",
      "Time (s): 0.4766860008239746\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 233 / 242\n",
      "LR: 8.297908510253126e-05\n",
      "Train loss: 4.097280502319336\n",
      "\n",
      "Time (s): 0.5017001628875732\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 234 / 242\n",
      "LR: 8.315377791327344e-05\n",
      "Train loss: 4.026549339294434\n",
      "\n",
      "Time (s): 0.47717809677124023\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 235 / 242\n",
      "LR: 8.33284707240156e-05\n",
      "Train loss: 4.080798625946045\n",
      "\n",
      "Time (s): 0.48290157318115234\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 236 / 242\n",
      "LR: 8.350316353475777e-05\n",
      "Train loss: 4.078069686889648\n",
      "\n",
      "Time (s): 0.4819207191467285\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 237 / 242\n",
      "LR: 8.367785634549994e-05\n",
      "Train loss: 4.0553202629089355\n",
      "\n",
      "Time (s): 0.46909046173095703\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 238 / 242\n",
      "LR: 8.385254915624211e-05\n",
      "Train loss: 4.009485721588135\n",
      "\n",
      "Time (s): 0.47568249702453613\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 239 / 242\n",
      "LR: 8.402724196698429e-05\n",
      "Train loss: 4.065518379211426\n",
      "\n",
      "Time (s): 0.4718940258026123\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 240 / 242\n",
      "LR: 8.420193477772646e-05\n",
      "Train loss: 4.0833563804626465\n",
      "\n",
      "Time (s): 0.47156500816345215\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 241 / 242\n",
      "LR: 8.437662758846863e-05\n",
      "Train loss: 4.17258882522583\n",
      "\n",
      "Time (s): 0.47588109970092773\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 2  Batch 242 / 242\n",
      "LR: 8.45513203992108e-05\n",
      "Train loss: 3.9782514572143555\n",
      "\n",
      "Time (s): 0.47779345512390137\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Evaluating:\n",
      "Epoch: 2\n",
      "Avg train loss: 4.012816279387671\n",
      "Avg train acc: 0.08477656223064611\n",
      "Avg eval loss: 4.011656655205621\n",
      "Avg eval acc: 0.08485260092549854\n",
      "=========================\n",
      "\n",
      "\n",
      "=========================\n",
      "NEW EPOCH: 3\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 1 / 242\n",
      "LR: 8.472601320995298e-05\n",
      "Train loss: 4.201966762542725\n",
      "\n",
      "Time (s): 0.539409875869751\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 2 / 242\n",
      "LR: 8.490070602069514e-05\n",
      "Train loss: 3.9952750205993652\n",
      "\n",
      "Time (s): 0.531688928604126\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 3 / 242\n",
      "LR: 8.507539883143731e-05\n",
      "Train loss: 4.058177947998047\n",
      "\n",
      "Time (s): 0.500128984451294\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 4 / 242\n",
      "LR: 8.525009164217948e-05\n",
      "Train loss: 3.932694911956787\n",
      "\n",
      "Time (s): 0.48833346366882324\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 5 / 242\n",
      "LR: 8.542478445292166e-05\n",
      "Train loss: 4.040748596191406\n",
      "\n",
      "Time (s): 0.4847283363342285\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 6 / 242\n",
      "LR: 8.559947726366383e-05\n",
      "Train loss: 4.063172340393066\n",
      "\n",
      "Time (s): 0.48183751106262207\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 7 / 242\n",
      "LR: 8.5774170074406e-05\n",
      "Train loss: 4.061092853546143\n",
      "\n",
      "Time (s): 0.4865841865539551\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 8 / 242\n",
      "LR: 8.594886288514817e-05\n",
      "Train loss: 4.10363245010376\n",
      "\n",
      "Time (s): 0.49031662940979004\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 9 / 242\n",
      "LR: 8.612355569589035e-05\n",
      "Train loss: 3.965832471847534\n",
      "\n",
      "Time (s): 0.48598575592041016\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 10 / 242\n",
      "LR: 8.629824850663251e-05\n",
      "Train loss: 4.141622543334961\n",
      "\n",
      "Time (s): 0.4914124011993408\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 11 / 242\n",
      "LR: 8.647294131737468e-05\n",
      "Train loss: 3.9518842697143555\n",
      "\n",
      "Time (s): 0.50419020652771\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 12 / 242\n",
      "LR: 8.664763412811685e-05\n",
      "Train loss: 4.067490100860596\n",
      "\n",
      "Time (s): 0.5049269199371338\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 13 / 242\n",
      "LR: 8.682232693885902e-05\n",
      "Train loss: 4.088957786560059\n",
      "\n",
      "Time (s): 0.5058927536010742\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 14 / 242\n",
      "LR: 8.69970197496012e-05\n",
      "Train loss: 3.9360945224761963\n",
      "\n",
      "Time (s): 0.48113083839416504\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 15 / 242\n",
      "LR: 8.717171256034337e-05\n",
      "Train loss: 4.032425403594971\n",
      "\n",
      "Time (s): 0.48195433616638184\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 16 / 242\n",
      "LR: 8.734640537108554e-05\n",
      "Train loss: 3.941685676574707\n",
      "\n",
      "Time (s): 0.4835665225982666\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 17 / 242\n",
      "LR: 8.75210981818277e-05\n",
      "Train loss: 4.069733619689941\n",
      "\n",
      "Time (s): 0.48435282707214355\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 18 / 242\n",
      "LR: 8.769579099256988e-05\n",
      "Train loss: 4.135776042938232\n",
      "\n",
      "Time (s): 0.5041728019714355\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 19 / 242\n",
      "LR: 8.787048380331205e-05\n",
      "Train loss: 4.031897068023682\n",
      "\n",
      "Time (s): 0.49487853050231934\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 20 / 242\n",
      "LR: 8.804517661405422e-05\n",
      "Train loss: 4.057138919830322\n",
      "\n",
      "Time (s): 0.4902160167694092\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 21 / 242\n",
      "LR: 8.821986942479639e-05\n",
      "Train loss: 4.078776836395264\n",
      "\n",
      "Time (s): 0.4835777282714844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 22 / 242\n",
      "LR: 8.839456223553857e-05\n",
      "Train loss: 4.1392903327941895\n",
      "\n",
      "Time (s): 0.4872250556945801\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 23 / 242\n",
      "LR: 8.856925504628074e-05\n",
      "Train loss: 4.210435390472412\n",
      "\n",
      "Time (s): 0.4889709949493408\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 24 / 242\n",
      "LR: 8.87439478570229e-05\n",
      "Train loss: 4.100557804107666\n",
      "\n",
      "Time (s): 0.4842655658721924\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 25 / 242\n",
      "LR: 8.891864066776507e-05\n",
      "Train loss: 4.059237003326416\n",
      "\n",
      "Time (s): 0.5005412101745605\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 26 / 242\n",
      "LR: 8.909333347850726e-05\n",
      "Train loss: 4.04079008102417\n",
      "\n",
      "Time (s): 0.4834096431732178\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 27 / 242\n",
      "LR: 8.926802628924942e-05\n",
      "Train loss: 4.097081184387207\n",
      "\n",
      "Time (s): 0.47916650772094727\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 28 / 242\n",
      "LR: 8.944271909999159e-05\n",
      "Train loss: 4.139027118682861\n",
      "\n",
      "Time (s): 0.480226993560791\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 29 / 242\n",
      "LR: 8.961741191073376e-05\n",
      "Train loss: 4.019309043884277\n",
      "\n",
      "Time (s): 0.4879791736602783\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 30 / 242\n",
      "LR: 8.979210472147593e-05\n",
      "Train loss: 4.008829593658447\n",
      "\n",
      "Time (s): 0.48296141624450684\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 31 / 242\n",
      "LR: 8.996679753221811e-05\n",
      "Train loss: 3.9144020080566406\n",
      "\n",
      "Time (s): 0.5035901069641113\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 32 / 242\n",
      "LR: 9.014149034296028e-05\n",
      "Train loss: 4.045975685119629\n",
      "\n",
      "Time (s): 0.4935128688812256\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 33 / 242\n",
      "LR: 9.031618315370244e-05\n",
      "Train loss: 4.097970962524414\n",
      "\n",
      "Time (s): 0.48209047317504883\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 34 / 242\n",
      "LR: 9.049087596444461e-05\n",
      "Train loss: 3.8591036796569824\n",
      "\n",
      "Time (s): 0.48389768600463867\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 35 / 242\n",
      "LR: 9.06655687751868e-05\n",
      "Train loss: 4.1025824546813965\n",
      "\n",
      "Time (s): 0.48084521293640137\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 36 / 242\n",
      "LR: 9.084026158592896e-05\n",
      "Train loss: 3.945161819458008\n",
      "\n",
      "Time (s): 0.48511409759521484\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 37 / 242\n",
      "LR: 9.101495439667113e-05\n",
      "Train loss: 3.8495304584503174\n",
      "\n",
      "Time (s): 0.4837672710418701\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 38 / 242\n",
      "LR: 9.11896472074133e-05\n",
      "Train loss: 4.010180950164795\n",
      "\n",
      "Time (s): 0.485933780670166\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 39 / 242\n",
      "LR: 9.136434001815548e-05\n",
      "Train loss: 3.9421093463897705\n",
      "\n",
      "Time (s): 0.4832746982574463\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 40 / 242\n",
      "LR: 9.153903282889765e-05\n",
      "Train loss: 3.9434192180633545\n",
      "\n",
      "Time (s): 0.488095760345459\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 41 / 242\n",
      "LR: 9.171372563963981e-05\n",
      "Train loss: 4.115962028503418\n",
      "\n",
      "Time (s): 0.4835317134857178\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 42 / 242\n",
      "LR: 9.188841845038198e-05\n",
      "Train loss: 3.9693706035614014\n",
      "\n",
      "Time (s): 0.4836723804473877\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 43 / 242\n",
      "LR: 9.206311126112416e-05\n",
      "Train loss: 4.173954486846924\n",
      "\n",
      "Time (s): 0.4857912063598633\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 44 / 242\n",
      "LR: 9.223780407186633e-05\n",
      "Train loss: 4.017189025878906\n",
      "\n",
      "Time (s): 0.5498752593994141\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 45 / 242\n",
      "LR: 9.24124968826085e-05\n",
      "Train loss: 4.165120601654053\n",
      "\n",
      "Time (s): 0.48404955863952637\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 46 / 242\n",
      "LR: 9.258718969335067e-05\n",
      "Train loss: 3.9521796703338623\n",
      "\n",
      "Time (s): 0.4871227741241455\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 47 / 242\n",
      "LR: 9.276188250409284e-05\n",
      "Train loss: 3.9981884956359863\n",
      "\n",
      "Time (s): 0.4820671081542969\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 48 / 242\n",
      "LR: 9.293657531483502e-05\n",
      "Train loss: 3.9878451824188232\n",
      "\n",
      "Time (s): 0.48772287368774414\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 49 / 242\n",
      "LR: 9.311126812557718e-05\n",
      "Train loss: 4.031776428222656\n",
      "\n",
      "Time (s): 0.48571157455444336\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 50 / 242\n",
      "LR: 9.328596093631935e-05\n",
      "Train loss: 4.0504937171936035\n",
      "\n",
      "Time (s): 0.4863698482513428\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 51 / 242\n",
      "LR: 9.346065374706152e-05\n",
      "Train loss: 3.9144766330718994\n",
      "\n",
      "Time (s): 0.4912605285644531\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 52 / 242\n",
      "LR: 9.36353465578037e-05\n",
      "Train loss: 4.024137496948242\n",
      "\n",
      "Time (s): 0.492323637008667\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 53 / 242\n",
      "LR: 9.381003936854587e-05\n",
      "Train loss: 3.998931884765625\n",
      "\n",
      "Time (s): 0.48985910415649414\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 54 / 242\n",
      "LR: 9.398473217928804e-05\n",
      "Train loss: 3.9615635871887207\n",
      "\n",
      "Time (s): 0.48870015144348145\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 55 / 242\n",
      "LR: 9.41594249900302e-05\n",
      "Train loss: 4.08761739730835\n",
      "\n",
      "Time (s): 0.4854128360748291\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 56 / 242\n",
      "LR: 9.433411780077239e-05\n",
      "Train loss: 4.069149971008301\n",
      "\n",
      "Time (s): 0.4859468936920166\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 57 / 242\n",
      "LR: 9.450881061151455e-05\n",
      "Train loss: 4.015676975250244\n",
      "\n",
      "Time (s): 0.48507094383239746\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 58 / 242\n",
      "LR: 9.468350342225672e-05\n",
      "Train loss: 4.059321880340576\n",
      "\n",
      "Time (s): 0.4858970642089844\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 59 / 242\n",
      "LR: 9.485819623299889e-05\n",
      "Train loss: 4.059101104736328\n",
      "\n",
      "Time (s): 0.4869701862335205\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 60 / 242\n",
      "LR: 9.503288904374107e-05\n",
      "Train loss: 4.002540111541748\n",
      "\n",
      "Time (s): 0.4854872226715088\n",
      "=========================\n",
      "\n",
      "=========================\n",
      "Epoch 3  Batch 61 / 242\n",
      "LR: 9.520758185448324e-05\n",
      "Train loss: 4.254120826721191\n",
      "\n",
      "Time (s): 0.5007927417755127\n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Start to Train the Model\n",
    "batch_size = 4 #@param {type:\"slider\", min:0, max:8, step:1}\n",
    "\n",
    "#number_of_training_epochs = 150 #@param {type:\"slider\", min:0, max:200, step:1}\n",
    "number_of_training_epochs = 50 #@param {type:\"slider\", min:0, max:200, step:1}\n",
    "\n",
    "maximum_output_MIDI_sequence = 2048 #@param {type:\"slider\", min:0, max:8192, step:128}\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "strt = time.asctime( time.localtime( time.time() ) )\n",
    "\n",
    "!python3 train.py -output_dir rpr --rpr -batch_size=$batch_size -epochs=$number_of_training_epochs -max_sequence=$maximum_output_MIDI_sequence #-n_layers -num_heads -d_model -dim_feedforward\n",
    "\n",
    "print(strt)\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9VLdhokSGUAu"
   },
   "outputs": [],
   "source": [
    "#@title Re-Start Training from a certain checkpoint and epoch\n",
    "batch_size = 4 #@param {type:\"slider\", min:0, max:8, step:1}\n",
    "number_of_training_epochs = 150 #@param {type:\"slider\", min:0, max:200, step:1}\n",
    "maximum_output_MIDI_sequence = 2048 #@param {type:\"slider\", min:0, max:8192, step:128}\n",
    "\n",
    "saved_checkpoint_full_path = \"/home/mnt3p22/MusicTransformer-Pytorch/rpr/weights/epoch_0033.pickle\" #@param {type:\"string\"}\n",
    "#saved_checkpoint_full_path = \"/content/MusicTransformer-Pytorch/rpr/weights/epoch_0033.pickle\" #@param {type:\"string\"}\n",
    "\n",
    "continue_epoch_number = 33 #@param {type:\"integer\"}\n",
    "\n",
    "!python3 train.py -output_dir rpr --rpr -batch_size=$batch_size -epochs=$number_of_training_epochs -max_sequence=$maximum_output_MIDI_sequence -continue_weights $saved_checkpoint_full_path -continue_epoch $continue_epoch_number #-n_layers -num_heads -d_model -dim_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1D-o-E-TnI8"
   },
   "source": [
    "###Evaluate the resulted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "qQLOmv7wrOos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'evaluate.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#@title Evaluate Best Resulting Accuracy Model (best_acc_weights.pickle)\n",
    "!python3 evaluate.py -model_weights rpr/results/best_acc_weights.pickle --rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "c7QftGOfTyx2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'evaluate.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#@title Evaluate Best Resulting Loss Model (best_loss_weights.pickle)\n",
    "!python3 evaluate.py -model_weights rpr/results/best_loss_weights.pickle --rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "MusrrrOxt1uy"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoYUlEQVR4nO3deXxcdb3/8ddnlmSSydokXdM2XaCFlpZihQK9sqk/VETFBVHUqyjqVQFxw3vdQP35c0NQ9CoqiwugIptw9VJlE0WgZSm0QAullXRNS9NszTqf3x9zEkNp0mmbyWTOvJ+Px3nMnDMz53wOpJ/5zvd8z+dr7o6IiIRPJNcBiIhIdijBi4iElBK8iEhIKcGLiISUEryISEgpwYuIhJQSvEiOmdlXzOxXuY5DwkcJXsYkM1tvZq/OwXGvMbNuM2szsxfNbJmZzR3F4zeYmZtZbLSOKeGlBC/yct9y9zJgCrAR+HmO4xE5IErwklfMrNjMLjOzTcFymZkVB6/VmtntZtYctL7/amaR4LXPmdlGM2s1s2fM7JR9HcvddwO/BY4cdPzJZvZ7M2sys+fN7LxBrx1tZsvNrMXMtprZpcH2E82scY/zGOoXyn3BY3PwK+JYM5ttZvea2S4z225mv9nP/2xSoPQzUPLNfwFLSCddB24FvgB8EfgU0AjUBe9dAriZzQE+DrzS3TeZWQMQ3deBzCwJnAU8G6xHgD8ExzwLqAf+bGbPuPv/ApcDl7v7L82sDJh/AOf3KuB5oMrde4PjXg/cCZwEFAGLD2C/UoDUgpd8827gEnff5u5NwMXAe4LXeoBJwHR373H3v3q62FIfUAwcbmZxd1/v7s8Nc4xPm1kz0AosHbT/VwJ17n6Ju3e7+zrgp8A7Bx1/tpnVunubu/9jhM65B5gOTHb3Tne/f4T2KyGnBC/5ZjKwYdD6hmAbwLdJt7bvNLN1ZnYRgLs/C1wAfAXYZmY3mNlkhvYdd68CGoDdwJxg+3RgctAF1Bx8CfwnMCF4/RzgUOBpM3vYzE47iPMc7LOAAQ+Z2Soz+8AI7VdCTgle8s0m0om237RgG+7e6u6fcveZwOnAhf197e5+nbsvDT7rwDf3dSB3/ydwPnC5mZUALwDPu3vVoKXc3V8fvH+tu58FjA/2f2PQzdMOlPbv18yi/Ksb6WWH3UscW9z9Q+4+Gfgw8CMzm72v+EWU4GUsi5tZYtASA64HvmBmdWZWC3wJ+BWAmZ0WXJA0YBfprpmUmc0xs5ODi7GdpFvlqUwCcPdlpL9AzgUeAlqDC7YlZhY1s/lm9srg+GebWZ27p4DmYBcpYA2QMLM3mFmc9DWD4iEO2RR8Zmb/BjN7u5nVB6s7SX8JZBS/FDYleBnL/od0Mu5fvgJ8DVgOrASeAB4JtgEcAvwZaAMeAH7k7neTTqb/D9gObCHdwv78fsTxbdLdJDHgNNIXeJ8P9vczoDJ436nAKjNrI33B9Z3uvtvddwH/Ebx3I+kW/UtG1fRz9w7g68Dfgm6gJaT7/h8M9nsbcH7Q/y8yLNOEHyIi4aQWvIhISCnBi4iElBK8iEhIKcGLiITUmCpVUFtb6w0NDbkOQ0Qkb6xYsWK7u+/1vooxleAbGhpYvnx5rsMQEckbZrZhqNfURSMiElJK8CIiIaUELyISUmOqD35venp6aGxspLOzM9ehHLBEIkF9fT3xeDzXoYhIARnzCb6xsZHy8nIaGhpI15DKL+7Ojh07aGxsZMaMGbkOR0QKyJjvouns7KSmpiYvkzuAmVFTU5PXv0BEJD+N+QQP5G1y75fv8YtIfsqLBD8cd2drSyetnT25DkVEZEzJ+wRvZmxv7aK1szfXoYiIjCl5n+ABohGjL5X9uvbuTiq174l0env1ZSMiuReaBN+bpQS/fv165syZw3vf+17mz59PNBrlk5/8JPPmzeOUU06hqakJgBNPPJELLriAxYsXc/nll2clFhGR/THmh0kOdvEfVrF6U8vLtnf29OFASTy63/s8fHIFX37jvGHfs3btWq699lqWLFmCmbF48WK+973vcckll3DxxRdzxRVXANDd3a1aOiIyZoSiBW9mZHPmwenTp7NkyRIAIpEIZ555JgBnn302999//8D7+reLiIwFedWCH6qlvbF5N80d3cybXLnX1w9WMpkc8rXBQyCHe5+IyGgLRQs+FlxkHY0JxFOpFDfeeCMA1113HUuXLs36MUVEDkRWE7yZVZnZjWb2tJk9ZWbHZuM40Ui6FT0aI2mSySQPPfQQ8+fP56677uJLX/pS1o8pInIgst1FcznwJ3d/m5kVAaXZOEh/gu9NObH9v846rIaGBp588smXbLv00ktf9r577rlnZA8sInKQspbgzawSeBXw7wDu3g10Z+NYsVFswYuI5ItsdtHMAJqAq83sUTP7mZll5SrkaHbRtLW1Zf0YIiIjIZsJPgYcBfy3uy8C2oGL9nyTmZ1rZsvNbHn/TUN72tfF08FdNGPRaFz8FRHZUzYTfCPQ6O4PBus3kk74L+HuV7r7YndfXFf38onBE4kEO3bsGDZJjuUumv568IlEItehiEiByVofvLtvMbMXzGyOuz8DnAKs3t/91NfX09jYyFCt+37bdu6mIxFje8nYmzWpf0YnEZHRlO1RNJ8Afh2MoFkHvH9/dxCPxzOaCek9X1vGaw6fyDfOOGz/oxQRCaGsJnh3fwxYnM1j9KsqLaK5IyuDdERE8lIo7mQFqC6Ns1MJXkRkQGgSfLoFr1mdRET6hSbBV5fGleBFRAYJTYKvKi1SF42IyCAhSvBxunpT7O7uy3UoIiJjQmgSfHVpEYBa8SIigRAl+PQNTkrwIiJpoUnwVUELXhdaRUTSQpPg1UUjIvJSIUrw/V00asGLiECIEvxAF027WvAiIhCiBF8Ui5AsiqoFLyISCE2CBxUcExEZLFQJvjoZp3m3WvAiIhC2BK9yBSIiA0KV4FVRUkTkX8KV4EtUE15EpF+oEnx1aZxdu3vG5OTbIiKjLVQJvqq0CHdo0YVWEZFwJfjqpAqOiYj0C1WCrxqoR6MWvIhIqBJ89UBFSbXgRURCluBVcExEpF+oEnyVWvAiIgNCleArEjGiEdNFVhERQpbgzYyqkrjuZhURIWQJHqCqVAleRARCmOBVcExEJC10Cb6qNK5RNCIihDLBa9IPEREIYYKvLlVFSRERCGGCryotorMnRWdPX65DERHJqdAl+OqBejRqxYtIYQthgg/KFbTrQquIFLbQJXiVKxARSYsN96KZ1QPvBP4NmAzsBp4E7gD+6O6prEe4n/5VE14teBEpbEMmeDO7GpgC3A58E9gGJIBDgVOB/zKzi9z9vtEINFMDJYN3qwUvIoVtuBb8d939yb1sfxK4ycyKgGnZCevAVQV98CpXICKFbsg++P7kbmZvNLOXvc/du9392WwGdyCKY1FKi6LsbFcLXkQKWyYXWc8E1prZt8xs7v7s3MzWm9kTZvaYmS0/sBD3X7oejVrwIlLYhr3ICuDuZ5tZBXAWcI2ZOXA1cL27t2ZwjJPcfftBxrlf0hUl1YIXkcKW0TBJd28BbgRuACYBbwEeMbNPZDG2A1alcgUiIvtO8GZ2upndDNwDxIGj3f11wELgU/v4uAN3mtkKMzt3iP2fa2bLzWx5U1PT/kU/hHTBMXXRiEhh22cXDfBW4Ht7Dod09w4zO2cfn13q7hvNbDywzMye3st+rgSuBFi8eLHvR+xDUsExEZEMWvDu/j5gTdCSf6OZTRz02l/28dmNweM24Gbg6IOMNyPVpUXs2t1DKjUi3xciInkpky6ac4CHgDOAtwH/MLMPZPC5pJmV9z8HXkt6DH3WVZUWkXJo6VQ3jYgUrky6aD4LLHL3HQBmVgP8HbhqH5+bANxsZv3Huc7d/3QQsWZsoOBYR89AbRoRkUKTSYLfAQweDtkabBuWu68jfSF21A0uGTyDZC5CEBHJuUwS/LPAg2Z2K+lRMW8CVprZhQDufmkW4zsg/eUKdmkkjYgUsEwS/HPB0u/W4LF85MMZGZr0Q0QksztZLwYws7JgvS3bQR2sfyV4teBFpHBlMopmvpk9CqwCVgU3Lc3LfmgHrjwRI2Ka9ENEClsmpQquBC509+nuPp303as/zW5YBycSMapKi9RFIyIFLZMEn3T3u/tX3P0eGPtDU6pK4uqiEZGClslF1nVm9kXgl8H62cC67IU0MlRRUkQKXSYt+A8AdcBNwO+B2mDbmFZdWsTOdrXgRaRw7WvS7Shwk7ufNErxjJiq0iKe2tyS6zBERHJm2Ba8u/cBKTOrHKV4Rky6oqRa8CJSuDLpg28DnjCzZUB7/0Z3Py9rUY2A6mQRu3v66OzpIxGP5jocEZFRl0mCvylYBhvzdXgHyhXs7lGCF5GClEmCr3L3ywdvMLPzsxTPiBlcrmBCRSLH0YiIjL5MRtG8by/b/n2E4xhx/S14jaQRkUI1ZAvezM4C3gXMMLPbBr1UDryY7cAOVn8LXmPhRaRQDddF83dgM+lx798dtL0VWJnNoEaCCo6JSKEbMsG7+wZgA3Ds6IUzcga6aNSCF5EClUk1yTPMbK2Z7TKzFjNrNbMxfwdRIh4lEY+oi0ZEClYmo2i+BbzR3Z/KdjAjrSZZzNaWrlyHISKSE5mMotmaj8kdYNG0Kv6xbgfuY37YvojIiMskwS83s9+Y2VlBd80ZZnZG1iMbASccWse21i6e3tK67zeLiIRMJl00FUAH8NpB25yX39065pxwaB0A965p4rBJFTmORkRkdGUyJ+v7RyOQbBhfkeCwSRXc+0wTHzlhVq7DEREZVUN20ZjZbwc9/+Yer92ZzaBG0gmH1rF8w4u0dfXmOhQRkVE1XB/8IYOev2aP1+qyEEtWnHBoHT19zgPP7ch1KCIio2q4BD/c0JO8GZbyiunVJIui3LtmW65DEREZVcP1wZea2SLSXwIlwXMLlpLRCG4kFMUiHDe7lnueacLdMbNchyQiMiqGS/CbgUuD51sGPe9fzxsnHFrHstVbeX57OzPrynIdjojIqBiuFk3ezcM6lMHDJZXgRaRQZHKjU96bOq6UmXVJ7l3TlOtQRERGTUEkeEi34v+xbgedPX25DkVEZFQUVILv7Enx0PNjfq4SEZERkUm54OPNLBk8P9vMLjWz6dkPbWQtmVlDcSyibhoRKRiZtOD/G+gws4XAp4DngF9kNaosSMSjHDOzRgleRApGJgm+19P1dt8EXOHuPyQ9L2veOeHQOp7d1kbjzo5chyIiknWZJPhWM/s8cDZwh5lFgHh2w8qO/uGS963ZnuNIRESyL5MEfybQBZzj7luAeuDbWY0qS2bVJZlSVaKyBSJSEDKpB98KXO7ufWZ2KDAXuD67YWWHmXHCnDpue2wTPX0p4tGCGUQkIgUokwx3H1BsZlOAO4H3ANdkegAzi5rZo2Z2+4GFOLJOOLSOtq5eHtmwM9ehiIhkVSYJ3ty9AzgD+JG7vx2Yvx/HOB8YM3O6HjerhljEuPsZjaYRkXDLKMGb2bHAu4E79uNzmFk98AbgZwcW3sgrT8R51aF1XP235/n7s7rYKiLhlUmivgD4PHCzu68ys5nA3Rnu/zLgs0BqqDeY2blmttzMljc1jU6r+rtvX0hDTZJzrl3Ow+t1Z6uIhNM+E7y73+vupwM/NLMyd1/n7uft63Nmdhqwzd1X7GP/V7r7YndfXFc3OhNFVSeL+NUHj2FSVYL3X/0wj73QPCrHFREZTZmUKjjCzB4FVgGrzWyFmc3LYN/HA6eb2XrgBuBkM/vVQUU7gurKi7nug0sYlyzivT9/kFWbduU6JBGREZVJF81PgAvdfbq7TyNdruCn+/qQu3/e3evdvQF4J3CXu599UNGOsImVCa770DGUFcd4z88fYs3W1lyHJCIyYjJJ8El3H+hzd/d7gGTWIhpl9dWlXPehJcQixrt/9iDPb2/PdUgiIiMikwS/zsy+aGYNwfIFYN3+HMTd73H30w4sxOxrqE1y3YeOIZVy3vrff+fuZ3Snq4jkv0wS/AeAOuAm4PdAbbAtVGaPL+e3HzmW8eXFvP/qh/na7avp7h1y8I+IyJg3bKkCM4sCN4VpftbhzKor45aPHc/X73iKn93/PA+tf5Hvv3MRDbWh6ZESkQIybAve3fuAlJlVjlI8OZeIR/nqm+fz47Nfwfrt7Zz2g/u59bGNuQ5LRGS/ZVJsrA14wsyWAQNXIDMZC5/PTp0/kSPqKzn/+kc5/4bHWL5+J5e8aR5mluvQREQykkmCvylYCs6UqhJuOHcJ//d/nuaqvz3P3EnlvPuYvJutUEQK1JAJ3szqgDp3v3aP7fOAghlmEotG+MIbDuPZpjYu+cNqXjG9mrkTK3IdlojIPg3XB/8D0iNm9jQOuDw74YxNkYhx6TsWUlES52O/foSO7t5chyQisk/DJfjZ7n7fnhvd/a/AguyFNDbVlhVz2ZlHsm57O1++dVWuwxER2afhEvxwE2vn5ZysB+v42bV8/KTZ/G5FI7c8qpE1IjK2DZfgnzWz1++50cxex37eyRom559yCEc3jOO/bn5CZQ1EZEwbLsF/ErjMzK4xs08Ey7Wk+9/PH53wxp5YNMLlZx1JPBbh49c9QldvX65DEhHZqyETvLuvAY4A7gUaguVeYEHwWsGaVFnCd962kFWbWvjG/zyd63BERPZquGGS5u5dwNX7eI9nJbIx7tWHT+C9x07n2gfW8+ETZjKpsiTXIYmIvMRwXTR3B90y0wZvNLMiMzs56K55X3bDG9vOWToDd7jl0U25DkVE5GWGS/CnAn3A9Wa2ycxWm9nzwFrgLOAyd79mFGIcs6bXJFk8vZqbHmmkQH/IiMgYNlwffKe7/8jdjwemA6cAi4KZnT7k7o+OWpRj2BlH1bN2WxtPbmzJdSgiIi+RST143L3H3Te7e3OW48k7bzhiEkXRCL9/pDHXoYiIvERGCV6GVlka59WHj+cPj2+ip08ThIjI2KEEPwLOWFTPjvZu7lvTlOtQREQG7DPBm1nSzCLB80PN7HQzK8hSBUM5YU4d45JF3PSIyheIyNiRSQv+PiBhZlOAO4H3ANdkM6h8E49GOH3hZJY9tZVdHT25DkdEBMgswZu7dwBnAD9y97cD87IbVv4546gpdPemuOOJzbkORUQEyDDBm9mxwLuBO4Jt0eyFlJ+OmFLJ7PFl3PyoRtOIyNiQSYK/APg8cLO7rzKzmcDdWY0qD5kZZxw1hYfX72TDDlWZFJHc22eCd/d73f10d/9mcLF1e9gn3D5Qbz5yCmZws2rFi8gYkMkomuvMrMLMksCTwGoz+0z2Q8s/k6tKOHZmDTc/ulGlC0Qk5zLpojnc3VuANwN/BGaQHkkje3HGUfVs2NHBI//cmetQRKTAZZLg48G49zcDt7l7D6Dm6RBOnT+RkniU32tMvIjkWCYJ/ifAeiAJ3Gdm0wFV1hpCWXGMU+dP5A+Pb6KptSvX4YhIAcvkIuv33X2Ku7/e0zYAJ41CbHnrg/82g56+FB+45mHaunpzHY6IFKhMLrJWmtmlZrY8WL5LujUvQ5g3uZIfvusoVm9u4aO/WkF3r4qQicjoy6SL5iqgFXhHsLQwzDR+knbKYRP4xluO4K9rt/O5368kldJlCxEZXUPOyTrILHd/66D1i83ssSzFEyrveOVUtrV28p071zC+opjPv+6wXIckIgUkkwS/28yWuvv9AGZ2PLA7u2GFx8dOms3Wli5+cu86xpcnOGfpjFyHJCIFIpME/xHgF2ZWGazvpMAn294fZsZXTp9HU2sXX719NePLi3njwsm5DktECkAmo2ged/eFwAJggbsvAk7OemQhEo0Yl73zSI6eMY4Lf/sYv/zHBt3pKiJZl/GMTu7eEtzRCnBhluIJrUQ8yk/fu5jjZ9fyxVue5PwbHtMQShHJqgOdss9GNIoCUVkS56r3vZLP/J853L5yE6dfcT9Pb9E9YyKSHQea4PfZv2BmCTN7yMweN7NVZnbxAR4rVCIR42MnzebXH1xCa2cvb/7h3/jd8hdyHZaIhNCQCd7MWs2sZS9LK5DJVcIu4OSg//5I4FQzWzIyYee/Y2fVcMd5S1k0tZrP3LiSz/zucVo7Nd2fiIycIRO8u5e7e8VelnJ33+fom6CsQVuwGg8WXVkcZHx5gl998BjOO3k2Nz7SyMnfvZebHmnUBVgRGREH2kWTETOLBjdFbQOWufuDe3nPuf1lEJqamrIZzpgUjRgXvnYOt/zH8UyuKuHC3z7O23/8AE9u3JXr0EQkz9lotBbNrAq4GfiEuz851PsWL17sy5cvz3o8Y1Uq5dz4SCPf/OPT7Ozo5l3HTOPTr51DVWlRrkMTkTHKzFa4++K9vZbVFnw/d28mPY/rqaNxvHwViRjvWDyVuz59Iu89toHrH3qBk75zD39dW3i/bETk4GUtwZtZXdByx8xKgNcAT2freGFSWRLnK6fP447zljKhIsEHrnmY21duynVYIpJnstmCnwTcbWYrgYdJ98HfnsXjhc7ciRX85sPHsmhqNZ+4/lF++cD6XIckInkkk1o0B8TdVwKLsrX/QlFZEucX5xzNx697lC/euood7d2cf8ohmOleMxEZ3qj0wcvBScSj/Pjso3jbK+q57M9r+fJtq1RfXkT2KWsteBlZsWiEb79tAeOSRVx53zpebO/m0nccSVFM39EisndK8HnEzPjP1x9GTbKIb/zxaXr6UvzwXUcRiyrJi8jLKTPkoQ+fMIsvv/Fw/nfVVj57o6YDFJG9Uws+T73/+Bm0d/XynTvXUFIU5Wtvnq8LryLyEkrweexjJ82mrauPH9/7HGXFMS563VwleREZoASfx8yMz506h/auXn5y3zqSxTHOO+WQXIclImOEEnyeMzMuPn0e7d29XLpsDcnimCb2FhFACT4UIhHjW29dQEdXH1+9fTUbdrRzztIZTK9J5jo0EckhjaIJiVg0wuVnHclZR0/lugf/yYnfuYcP/WI5/1i3Q/XlRQrUqJQLzlShlwseKVtbOvnlAxv49YMb2NnRw7zJFXzg+BmctnASxbForsMTkRE0XLlgJfgQ293dxy2PbeSq+59n7bY2yotjvGbeBE5bMImls+t0F6xICCjBFzh352/P7uDWxzbyv6u20NLZS0UixmvnTeQNCyaxdHYtcd0NK5KXlOBlQHdvir89u53bV27mztVbaO3sZfb4Mr779oUsnFqV6/BEZD8pwctedfX28ZentvHV21ezrbWLj54wi/NOOURdNyJ5JOdT9snYVByL8vojJvGnC17FWxZN4Yq7n+X0K+5n9aaWXIcmIiNALXgZ8OfVW7nopido7ujmvFMO4ZylM9jZ0U1Taxfb2/ofuxiXLOIti6aQLNZtFCK5pi4aydjO9m6+dNsq/vD48HPAVpbEOXvJNN53XAPjyxOjFJ2I7EkJXvbbX57aylObW6grL6a2rJi68vRSkyzmyU27+Ol96/jTqi3EIxHesmgKH3rVDGaPL8912CIFRwlesmL99nZ+dv86fre8ka7eFP92SC2nzB3PiXPG01CrMgkio0EJXrJqR1sXv3hgA7c+tpH1OzoAmF5TygmH1nHCoXUcO6uG0iL114tkgxK8jJr129u5b20T9z7TxN+f28Hunj7iUePIqVUcO7OGJbNqOGpaNYm4SiaIjAQleMmJrt4+lq/fyV/XbueBdTt4orGZlENRLMKiqVUcM7OGI6ZUMm9yBZMqE5qsROQADJfg9btZsqY4FuX42bUcP7sWgNbOHh5e/yIPPLeDB9bt4Ad3raW/fVFVGufwSRUcPqmCeVMqOGJKFTNrk0QiSvoiB0oJXkZNeSLOyXMncPLcCQC0d/Xy9JZWVm9uYfWmXaze1MIv/7GBrt5U+v3FMeZPqWTB1EoW1lexoL6SKVUlaumLZEgJXnImWRzjFdOrecX06oFtvX0pnmtqZ2VjM483NrOycRdX3f88PX3ppn5tWREL66tYODVY6iupKi3K1SmIjGlK8DKmxKIR5kwsZ87Ect6+eCqQ7st/enMrjzc28/gLu1jZ2Mxdz2wb6N6ZXlPKIePLmVWXZFZdGTPrksysK2NcUolfCpsSvIx5xbHoQIudY9PbWjt7eGLjroGE/1xTG/etaaK7LzXwuerSOPOnVPKK6dUcNa2aI6dVUZGI5+YkRHJACV7yUnkiznGzajluVu3Atr6U07izg3VN7TzX1Maz29p47IVmLv9L+mKuGcyZUM6iadUsqK/ksEkVzJlQTkmRhmxKOGmYpIRea2cPj7+wixUbdrLinzt59J87ae3sBSBi0FCb5LBgBE9DTZIJFcVMqEgwvqJYUxzKmKdhklLQyhNxlh5Sy9JD0q39VMpp3Lmb1ZtbeCpYVjY2c8fKzS/77LhkEePLi6mvLlUfv+QdJXgpOJGIMa2mlGk1pZw6f+LA9tbOHjY272ZrSxdbd3WypaWTrcHyzxfb99rHP7OujJm1SWbUJZlZW8asuiTTakrV8pcxQQleJFCeiDN3Ypy5E/f++p59/M81tbOuqY171jTxuxWNA++LGEyoSFBZEqeiJE7loKU8EaMoFqEoGqE4FqE4FqUoFiERj1BaFCNZHKOsOEayOEpZ8Dym+XLlACnBi2QoGjGm1ySZXpPkpLnjX/JaS2cP67e3sy5I+pt2dbJrdw+7dvfwwosdrAqet3f37fdxyxMxxiWLqCotYlxpnOpkETXJIqbXJAd+PUysUKkHeTkleJERUJGIs6C+igX1VcO+L5VyuvtSdPWm6O5N0dXbR3dvis6eFO3dvbR19dIeLG1dfbR19rKzo5udHd282N5NU1sXa7a2sb2ta+COX4CSeJQZtUlm1CapKo1Tnkj/WqhIxChLxCgvTq/3by9P6NdBIVCCFxlFkYiRiEQPuppmKuVsaenk+e3trNue/tXw/PZ2Vm9uYdfuHlo7ewbu/h1OaVE6lqJoJN11FHQfFcUiRIeoA1RbVjQw6uiwSRXUV6t8xFilBC+ShyIRY3JVCZOrSgaKue2ps6eP1s70r4LWzh5aO3uD5aXPuwb9kuju6/9lkWJvI6gdZ+3WNu5cvXXg9YpEjLmTKphQkSCVcvpSTp8Hjykn5Y57+rOpVPDoUByLUFdezPjyxL+GppanH2vLioe9P8Hd2dnRw6bm3ezs6OaQ8eVMrNTUkXtSghcJqUQ83TqvKy8e8X13dKcLxT21uYXVm1pYvbmFJxqbiUYsWCJEIxA1w8yIGETMMAMzw4DWzl7WNbWzrbVzr782yopj1JYVDUwbWVoUY1trJxubd7OpeTedPamXvH98eTEL6tP1iRZMreKwSeX09jm7dvfQ3NHDrt3dwa+bXipK4gNfJhMqElSXxl/2K8Q93Z3W2ZOiq6ePrt4UnT196fXe9HpFIs60mlIqS8bmHdJZS/BmNhX4BTABcOBKd788W8cTkdFTWhTjqGnpEhAHK5VydnZ0s7Wli22tnWxr6aKprYvtbV1sb+umqbWTtdvaaOvsZUJlgrkTyzl5zngmV5UwpbqE8kSMZ7a0srJxF483NvPnp7budwzxqFFXlv4i7AwS+e6evr3+itmb6tI402qSNNSUMn1cKVOqSxhfnkj/QqlIz2Xc3+W1q6OH57a3DVyQf66pjd4+5+f//sr9jntfstmC7wU+5e6PmFk5sMLMlrn76iweU0TyTCRi1JQVU1NWzOFUHNA+BpesaOns4cnGXazZ2koiHk0PUS1ND1OtKi2irDhGy+4etrZ0sq21a+BxW0sXZpCIR0jEopQE1yeKY5G9PhbFIjR3dLNhRwcbXuzgnzs6WLFhJ394fBOpPb4YohGjJllEX8rZ0d49sD0W3JMxd2I57j7i1zKyluDdfTOwOXjeamZPAVMAJXgRyZqKRJzjZtdy3BDXJgAqS+JMHVealeN396bSv0Rau9g26Mtja0snETNmjU/fFDezLsnUcaXEsziSaVT64M2sAVgEPLiX184FzgWYNm3aaIQjIpI1RbEI9dWl1Fdn5wtkf2R9EKyZlQG/By5w95Y9X3f3K919sbsvrqury3Y4IiIFI6sJ3szipJP7r939pmweS0REXiprCd7SVwt+Djzl7pdm6zgiIrJ32WzBHw+8BzjZzB4Lltdn8XgiIjJINkfR3A/o/mURkRxRpSERkZBSghcRCSkleBGRkBpTk26bWROw4QA/XgtsH8Fw8oHOOfwK7XxB57y/prv7Xm8iGlMJ/mCY2fKhZhYPK51z+BXa+YLOeSSpi0ZEJKSU4EVEQipMCf7KXAeQAzrn8Cu08wWd84gJTR+8iIi8VJha8CIiMogSvIhISOV9gjezU83sGTN71swuynU82WBmV5nZNjN7ctC2cWa2zMzWBo8HPznmGGJmU83sbjNbbWarzOz8YHtoz9vMEmb2kJk9HpzzxcH2GWb2YPA3/hszK8p1rCPJzKJm9qiZ3R6sh/p8AcxsvZk9ERRhXB5sG/G/7bxO8GYWBX4IvA44HDjLzA7PbVRZcQ1w6h7bLgL+4u6HAH8J1sOkf07fw4ElwMeC/7dhPu8u4GR3XwgcCZxqZkuAbwLfc/fZwE7gnNyFmBXnA08NWg/7+fY7yd2PHDT+fcT/tvM6wQNHA8+6+zp37wZuAN6U45hGnLvfB7y4x+Y3AdcGz68F3jyaMWWbu29290eC562kE8AUQnzentYWrMaDxYGTgRuD7aE6ZzOrB94A/CxYN0J8vvsw4n/b+Z7gpwAvDFpvDLYVggnBxOYAW4AJuQwmm/aY0zfU5x10VzwGbAOWAc8Bze7eG7wlbH/jlwGfBVLBeg3hPt9+DtxpZiuCeakhC3/bozLptmSXu7uZhXK8655z+qYbeGlhPG937wOONLMq4GZgbm4jyh4zOw3Y5u4rzOzEHIcz2pa6+0YzGw8sM7OnB784Un/b+d6C3whMHbReH2wrBFvNbBJA8Lgtx/GMuCHm9A39eQO4ezNwN3AsUGVm/Y2xMP2NHw+cbmbrSXevngxcTnjPd4C7bwwet5H+Ij+aLPxt53uCfxg4JLjqXgS8E7gtxzGNltuA9wXP3wfcmsNYRtwwc/qG9rzNrC5ouWNmJcBrSF97uBt4W/C20Jyzu3/e3evdvYH0v9273P3dhPR8+5lZ0szK+58DrwWeJAt/23l/J2swz+tlQBS4yt2/ntuIRp6ZXQ+cSLqk6Fbgy8AtwG+BaaRLLL/D3fe8EJu3zGwp8FfgCf7VP/ufpPvhQ3neZraA9MW1KOnG12/d/RIzm0m6hTsOeBQ42927chfpyAu6aD7t7qeF/XyD87s5WI0B17n7182shhH+2877BC8iInuX7100IiIyBCV4EZGQUoIXEQkpJXgRkZBSghcRCSkleAk9M+sLqvb1LyNWoMzMGgZX+RQZS1SqQArBbnc/MtdBiIw2teClYAU1ub8V1OV+yMxmB9sbzOwuM1tpZn8xs2nB9glmdnNQr/1xMzsu2FXUzH4a1HC/M7gLFTM7L6hnv9LMbsjRaUoBU4KXQlCyRxfNmYNe2+XuRwBXkL4jGuAHwLXuvgD4NfD9YPv3gXuDeu1HAauC7YcAP3T3eUAz8NZg+0XAomA/H8nOqYkMTXeySuiZWZu7l+1l+3rSE2ysCwqbbXH3GjPbDkxy955g+2Z3rzWzJqB+8G3zQSnjZcEkDZjZ54C4u3/NzP4EtJEuK3HLoFrvIqNCLXgpdD7E8/0xuE5KH/+6tvUG0jOOHQU8PKhCosioUIKXQnfmoMcHgud/J13dEODdpIueQXoatY/CwMQclUPt1MwiwFR3vxv4HFAJvOxXhEg2qUUhhaAkmCWp35/cvX+oZLWZrSTdCj8r2PYJ4Goz+wzQBLw/2H4+cKWZnUO6pf5RYDN7FwV+FXwJGPD9oMa7yKhRH7wUrKAPfrG7b891LCLZoC4aEZGQUgteRCSk1IIXEQkpJXgRkZBSghcRCSkleBGRkFKCFxEJqf8PkXw3BA0PNQ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Graph the results\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS_FILE = \"results.csv\"\n",
    "EPOCH_IDX = 0\n",
    "LR_IDX = 1\n",
    "EVAL_LOSS_IDX = 4\n",
    "EVAL_ACC_IDX = 5\n",
    "\n",
    "SPLITTER = '?'\n",
    "\n",
    "\n",
    "#def graph_results(input_dirs=\"/content/MusicTransformer-Pytorch/rpr/results\", output_dir=None, model_names=None, epoch_start=0, epoch_end=None):\n",
    "def graph_results(input_dirs=\"/home/mnt3p22/MusicTransformer-Pytorch/rpr/results\", output_dir=None, model_names=None, epoch_start=0, epoch_end=None):\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Author: Damon Gwinn\n",
    "    ----------\n",
    "    Graphs model training and evaluation data\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    input_dirs = input_dirs.split(SPLITTER)\n",
    "\n",
    "    if(model_names is not None):\n",
    "        model_names = model_names.split(SPLITTER)\n",
    "        if(len(model_names) != len(input_dirs)):\n",
    "            print(\"Error: len(model_names) != len(input_dirs)\")\n",
    "            return\n",
    "\n",
    "    #Initialize Loss and Accuracy arrays\n",
    "    loss_arrs = []\n",
    "    accuracy_arrs = []\n",
    "    epoch_counts = []\n",
    "    lrs = []\n",
    "\n",
    "    for input_dir in input_dirs:\n",
    "        loss_arr = []\n",
    "        accuracy_arr = []\n",
    "        epoch_count = []\n",
    "        lr_arr = []\n",
    "\n",
    "        f = os.path.join(input_dir, RESULTS_FILE)\n",
    "        with open(f, \"r\") as i_stream:\n",
    "            reader = csv.reader(i_stream)\n",
    "            next(reader)\n",
    "\n",
    "            lines = [line for line in reader]\n",
    "\n",
    "        if(epoch_end is None):\n",
    "            epoch_end = math.inf\n",
    "\n",
    "        epoch_start = max(epoch_start, 0)\n",
    "        epoch_start = min(epoch_start, epoch_end)\n",
    "\n",
    "        for line in lines:\n",
    "            epoch = line[EPOCH_IDX]\n",
    "            lr = line[LR_IDX]\n",
    "            accuracy = line[EVAL_ACC_IDX]\n",
    "            loss = line[EVAL_LOSS_IDX]\n",
    "\n",
    "            if(int(epoch) >= epoch_start and int(epoch) < epoch_end):\n",
    "                accuracy_arr.append(float(accuracy))\n",
    "                loss_arr.append(float(loss))\n",
    "                epoch_count.append(int(epoch))\n",
    "                lr_arr.append(float(lr))\n",
    "\n",
    "        loss_arrs.append(loss_arr)\n",
    "        accuracy_arrs.append(accuracy_arr)\n",
    "        epoch_counts.append(epoch_count)\n",
    "        lrs.append(lr_arr)\n",
    "\n",
    "    if(output_dir is not None):\n",
    "        try:\n",
    "            os.mkdir(output_dir)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % output_dir)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s\" % output_dir)\n",
    "\n",
    "    ##### LOSS #####\n",
    "    for i in range(len(loss_arrs)):\n",
    "        if(model_names is None):\n",
    "            name = None\n",
    "        else:\n",
    "            name = model_names[i]\n",
    "\n",
    "        #Create and save plots to output folder\n",
    "        plt.plot(epoch_counts[i], loss_arrs[i], label=name)\n",
    "        plt.title(\"Loss Results\")\n",
    "        plt.ylabel('Loss (Cross Entropy)')\n",
    "        plt.xlabel('Epochs')\n",
    "        fig1 = plt.gcf()\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    if(output_dir is not None):\n",
    "        fig1.savefig(os.path.join(output_dir, 'loss_graph.png'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ##### ACCURACY #####\n",
    "    for i in range(len(accuracy_arrs)):\n",
    "        if(model_names is None):\n",
    "            name = None\n",
    "        else:\n",
    "            name = model_names[i]\n",
    "\n",
    "        #Create and save plots to output folder\n",
    "        plt.plot(epoch_counts[i], accuracy_arrs[i], label=name)\n",
    "        plt.title(\"Accuracy Results\")\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        fig2 = plt.gcf()\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    if(output_dir is not None):\n",
    "        fig2.savefig(os.path.join(output_dir, 'accuracy_graph.png'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ##### LR #####\n",
    "    for i in range(len(lrs)):\n",
    "        if(model_names is None):\n",
    "            name = None\n",
    "        else:\n",
    "            name = model_names[i]\n",
    "\n",
    "        #Create and save plots to output folder\n",
    "        plt.plot(epoch_counts[i], lrs[i], label=name)\n",
    "        plt.title(\"Learn Rate Results\")\n",
    "        plt.ylabel('Learn Rate')\n",
    "        plt.xlabel('Epochs')\n",
    "        fig2 = plt.gcf()\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    if(output_dir is not None):\n",
    "        fig2.savefig(os.path.join(output_dir, 'lr_graph.png'))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "#graph_results('/content/MusicTransformer-Pytorch/rpr/results', model_names='rpr')    \n",
    "graph_results('/home/mnt3p22/MusicTransformer-Pytorch/rpr/results', model_names='rpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxHrTsFUdn-r"
   },
   "source": [
    "To have the model continue your custom MIDI enter the following into the custom_MIDI field below:\n",
    "\n",
    "-primer_file '/content/some_dir/some_seed_midi.mid'\n",
    "\n",
    "For example: -primer_file '/content/MusicTransformer-Pytorch/seed.mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJXWoBMWL3ph"
   },
   "source": [
    "# Generate and Explore the output :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "czNulONr4tB6"
   },
   "outputs": [],
   "source": [
    "#@title Generate, Plot, Graph, Save, Download, and Render the resulting output\n",
    "number_of_tokens_to_generate = 1023 #@param {type:\"slider\", min:1, max:2048, step:1}\n",
    "priming_sequence_length = 65 #@param {type:\"slider\", min:1, max:2048, step:8}\n",
    "maximum_possible_output_sequence = 2048 #@param {type:\"slider\", min:0, max:2048, step:8}\n",
    "\n",
    "#select_model = \"/content/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\" #@param [\"/content/MusicTransformer-Pytorch/rpr/results/best_acc_weights.pickle\", \"/content/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\"]\n",
    "select_model = \"/home/mnt3p22/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\" #@param [\"/content/MusicTransformer-Pytorch/rpr/results/best_acc_weights.pickle\", \"/content/MusicTransformer-Pytorch/rpr/results/best_loss_weights.pickle\"]\n",
    "\n",
    "custom_MIDI = \"\" #@param {type:\"string\"}\n",
    "\n",
    "import processor\n",
    "from processor import encode_midi, decode_midi\n",
    "\n",
    "!python generate.py -output_dir output -model_weights=$select_model --rpr -target_seq_length=$number_of_tokens_to_generate -num_prime=$priming_sequence_length -max_sequence=$maximum_possible_output_sequence $custom_MIDI #\n",
    "\n",
    "print('Successfully exported the output to output folder. To primer.mid and rand.mid')\n",
    "\n",
    "# set the src and play\n",
    "#FluidSynth(\"/content/font.sf2\").midi_to_audio('/content/MusicTransformer-Pytorch/output/rand.mid', '/content/MusicTransformer-Pytorch/output/output.wav')\n",
    "FluidSynth(\"/home/mnt3p22/font.sf2\").midi_to_audio('/home/mnt3p22/MusicTransformer-Pytorch/output/rand.mid', '/content/MusicTransformer-Pytorch/output/output.wav')\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download('/home/mnt3p22/MusicTransformer-Pytorch/output/rand.mid')\n",
    "files.download('/home/mnt3p22/MusicTransformer-Pytorch/output/primer.mid')\n",
    "#files.download('/content/MusicTransformer-Pytorch/output/rand.mid')\n",
    "#files.download('/content/MusicTransformer-Pytorch/output/primer.mid')\n",
    "\n",
    "\n",
    "\n",
    "#Audio('/content/MusicTransformer-Pytorch/output/output.wav')\n",
    "Audio('/home/mnt3p22/MusicTransformer-Pytorch/output/output.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IG48uyKGzcTI"
   },
   "outputs": [],
   "source": [
    "#@title Plot and Graph the Output :)\n",
    "graphs_length_inches = 18 #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "notes_graph_height = 6 #@param {type:\"slider\", min:0, max:20, step:1}\n",
    "highest_displayed_pitch = 92 #@param {type:\"slider\", min:1, max:128, step:1}\n",
    "lowest_displayed_pitch = 24 #@param {type:\"slider\", min:1, max:128, step:1}\n",
    "piano_roll_color_map = \"Blues\"\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('SVG')\n",
    "# For plotting\n",
    "import mir_eval.display\n",
    "import librosa.display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#midi_data = pretty_midi.PrettyMIDI('/content/MusicTransformer-Pytorch/output/rand.mid')\n",
    "midi_data = pretty_midi.PrettyMIDI('/home/mnt3p22/MusicTransformer-Pytorch/output/rand.mid')\n",
    "\n",
    "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
    "    # Use librosa's specshow function for displaying the piano roll\n",
    "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "\n",
    "\n",
    "\n",
    "roll = np.zeros([int(graphs_length_inches), 128])\n",
    "# Plot the output\n",
    "\n",
    "track = Multitrack('/home/mnt3p22/MusicTransformer-Pytorch/output/rand.mid', name='track')\n",
    "#track = Multitrack('/content/MusicTransformer-Pytorch/output/rand.mid', name='track')\n",
    "\n",
    "plt.figure(figsize=[graphs_length_inches, notes_graph_height])\n",
    "fig, ax = track.plot()\n",
    "fig.set_size_inches(graphs_length_inches, notes_graph_height)\n",
    "plt.figure(figsize=[graphs_length_inches, notes_graph_height])\n",
    "ax2 = plot_piano_roll(midi_data, int(lowest_displayed_pitch), int(highest_displayed_pitch))\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFMWSDq7ZKM2"
   },
   "source": [
    "### Save to Google Drive (Standard GD connect code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-A3ju-Fz0Eh"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Super_Piano_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
